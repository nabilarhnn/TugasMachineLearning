{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LQ3xqjibNqwn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a dummy dataset\n",
        "def generate_dummy_data(n_samples=1000):\n",
        "    np.random.seed(42)\n",
        "    data = {\n",
        "        'age': np.random.randint(18, 90, size=n_samples),\n",
        "        'cholesterol': np.random.uniform(150, 300, size=n_samples),\n",
        "        'blood_pressure': np.random.uniform(80, 180, size=n_samples),\n",
        "        'heart_rate': np.random.uniform(60, 120, size=n_samples),\n",
        "        'target': np.random.choice([0, 1], size=n_samples)  # Binary classification target\n",
        "    }\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "NJuN0ldvOA7I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy dataset\n",
        "df = generate_dummy_data()"
      ],
      "metadata": {
        "id": "O3AaRQlcOEYq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the dataset\n",
        "print(\"Dataset Overview:\")\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJh8gd4qOHAb",
        "outputId": "e331066d-936b-4f9b-f229-213c130a54bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Overview:\n",
            "   age  cholesterol  blood_pressure  heart_rate  target\n",
            "0   69   280.049064      154.342379   92.670346       1\n",
            "1   32   211.197674      141.194051   72.267653       0\n",
            "2   89   232.758389       90.593071   75.318545       1\n",
            "3   78   188.083285       95.156937   77.741680       0\n",
            "4   38   179.416965       89.375692   95.628149       0\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 5 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   age             1000 non-null   int64  \n",
            " 1   cholesterol     1000 non-null   float64\n",
            " 2   blood_pressure  1000 non-null   float64\n",
            " 3   heart_rate      1000 non-null   float64\n",
            " 4   target          1000 non-null   int64  \n",
            "dtypes: float64(3), int64(2)\n",
            "memory usage: 39.2 KB\n",
            "None\n",
            "               age  cholesterol  blood_pressure   heart_rate       target\n",
            "count  1000.000000  1000.000000     1000.000000  1000.000000  1000.000000\n",
            "mean     52.881000   225.231581      131.060368    90.471115     0.507000\n",
            "std      20.958915    42.956958       29.221928    17.024582     0.500201\n",
            "min      18.000000   150.035629       80.018840    60.121111     0.000000\n",
            "25%      34.750000   187.943415      105.344262    76.138585     0.000000\n",
            "50%      52.500000   226.995830      132.792165    91.119017     1.000000\n",
            "75%      71.000000   261.956156      156.129293   105.115002     1.000000\n",
            "max      89.000000   299.903025      179.971380   119.972635     1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch Dataset class\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "        self.features = torch.tensor(dataframe.iloc[:, :-1].values, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(dataframe['target'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "vU7Gq16rOK7u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and validation sets\n",
        "dataset = DummyDataset(df)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "iVa2LsRZOLqU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "i-UNdSD2ONet"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "hXygoF_9OPPw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "7ixL69B4ORKZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for features, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5QX2qjBOS8-",
        "outputId": "3914b7dd-67fd-4e9f-9a6b-6ba350589970"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.5159\n",
            "Epoch 2/10, Loss: 0.9432\n",
            "Epoch 3/10, Loss: 0.7656\n",
            "Epoch 4/10, Loss: 0.7280\n",
            "Epoch 5/10, Loss: 0.7236\n",
            "Epoch 6/10, Loss: 0.7076\n",
            "Epoch 7/10, Loss: 0.7004\n",
            "Epoch 8/10, Loss: 0.7049\n",
            "Epoch 9/10, Loss: 0.6962\n",
            "Epoch 10/10, Loss: 0.6990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation loop\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "with torch.no_grad():\n",
        "    for features, targets in val_loader:\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, targets)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV8d9nODOVEn",
        "outputId": "130fb2e4-94b0-42aa-e961-410a554a8fa1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.6863\n"
          ]
        }
      ]
    }
  ]
}