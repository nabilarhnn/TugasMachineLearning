{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nMAZeV1rzcFE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "def load_and_preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # One-hot encoding for categorical columns\n",
        "    data = pd.get_dummies(data, columns=['month', 'day'], drop_first=True)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = data.drop(columns=['area'])\n",
        "    y = data['area']\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train.values, y_test.values\n",
        "\n",
        "file_path = '/content/forestfires.csv'\n",
        "X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path)"
      ],
      "metadata": {
        "id": "PaphJ_bwzjkI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
      ],
      "metadata": {
        "id": "VYO_tLYazmG6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for batch processing\n",
        "def create_data_loader(X, y, batch_size):\n",
        "    dataset = TensorDataset(X, y)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Ds83ScUkzm_4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP regression model\n",
        "class MLPRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, activation_fn):\n",
        "        super(MLPRegressionModel, self).__init__()\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        for hidden_units in hidden_layers:\n",
        "            layers.append(nn.Linear(in_features, hidden_units))\n",
        "            layers.append(activation_fn)\n",
        "            in_features = hidden_units\n",
        "\n",
        "        layers.append(nn.Linear(in_features, 1))  # Output layer for regression\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "_LJHdZEdzpN3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate the model\n",
        "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            predictions.extend(outputs.view(-1).tolist())\n",
        "            actuals.extend(y_batch.view(-1).tolist())\n",
        "\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    return mse"
      ],
      "metadata": {
        "id": "2ZRLtJTIzrRn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration variables\n",
        "input_size = X_train.shape[1]\n",
        "hidden_layer_configs = [[4], [8], [16], [32], [64]]  # Example configurations\n",
        "activation_functions = {'Linear': nn.Identity(), 'Sigmoid': nn.Sigmoid(), 'ReLU': nn.ReLU(), 'Softmax': nn.Softmax(dim=1), 'Tanh': nn.Tanh()}\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
        "epochs_list = [1, 10, 25, 50, 100, 250]"
      ],
      "metadata": {
        "id": "PV42EdoazvpT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Hidden Layer Comparison\n",
        "results_hidden_layers = []\n",
        "for hidden_layers in hidden_layer_configs:\n",
        "    train_loader = create_data_loader(X_train_tensor, y_train_tensor, batch_size=32)\n",
        "    test_loader = create_data_loader(X_test_tensor, y_test_tensor, batch_size=32)\n",
        "\n",
        "    model = MLPRegressionModel(input_size, hidden_layers, nn.ReLU())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    mse = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=50)\n",
        "\n",
        "    results_hidden_layers.append({\n",
        "        'hidden_layers': hidden_layers,\n",
        "        'mse': mse\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3uK_lSczy0n",
        "outputId": "027da02c-70c5-4bd0-8673-48a684f75a4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 2189.3948\n",
            "Epoch [2/50], Loss: 2201.1967\n",
            "Epoch [3/50], Loss: 2169.8288\n",
            "Epoch [4/50], Loss: 2143.6415\n",
            "Epoch [5/50], Loss: 2133.8825\n",
            "Epoch [6/50], Loss: 2114.7646\n",
            "Epoch [7/50], Loss: 2092.9977\n",
            "Epoch [8/50], Loss: 2080.0424\n",
            "Epoch [9/50], Loss: 2067.1284\n",
            "Epoch [10/50], Loss: 2063.8888\n",
            "Epoch [11/50], Loss: 2208.7880\n",
            "Epoch [12/50], Loss: 2053.7382\n",
            "Epoch [13/50], Loss: 2051.4999\n",
            "Epoch [14/50], Loss: 2049.8351\n",
            "Epoch [15/50], Loss: 2050.4680\n",
            "Epoch [16/50], Loss: 2047.9937\n",
            "Epoch [17/50], Loss: 2065.0326\n",
            "Epoch [18/50], Loss: 2056.3279\n",
            "Epoch [19/50], Loss: 2046.4641\n",
            "Epoch [20/50], Loss: 2045.4589\n",
            "Epoch [21/50], Loss: 2049.9219\n",
            "Epoch [22/50], Loss: 2045.8998\n",
            "Epoch [23/50], Loss: 2043.4878\n",
            "Epoch [24/50], Loss: 2042.9164\n",
            "Epoch [25/50], Loss: 2055.0835\n",
            "Epoch [26/50], Loss: 2042.5762\n",
            "Epoch [27/50], Loss: 2043.6515\n",
            "Epoch [28/50], Loss: 2046.6125\n",
            "Epoch [29/50], Loss: 2042.0596\n",
            "Epoch [30/50], Loss: 2048.8635\n",
            "Epoch [31/50], Loss: 2039.7119\n",
            "Epoch [32/50], Loss: 2039.6179\n",
            "Epoch [33/50], Loss: 2062.7136\n",
            "Epoch [34/50], Loss: 2052.1843\n",
            "Epoch [35/50], Loss: 2171.9880\n",
            "Epoch [36/50], Loss: 2043.7493\n",
            "Epoch [37/50], Loss: 2039.3335\n",
            "Epoch [38/50], Loss: 2039.4533\n",
            "Epoch [39/50], Loss: 2046.6312\n",
            "Epoch [40/50], Loss: 2042.5039\n",
            "Epoch [41/50], Loss: 2052.7877\n",
            "Epoch [42/50], Loss: 2037.2069\n",
            "Epoch [43/50], Loss: 2052.1943\n",
            "Epoch [44/50], Loss: 2034.9451\n",
            "Epoch [45/50], Loss: 2034.3502\n",
            "Epoch [46/50], Loss: 2035.2106\n",
            "Epoch [47/50], Loss: 2165.4933\n",
            "Epoch [48/50], Loss: 2163.8395\n",
            "Epoch [49/50], Loss: 2033.3932\n",
            "Epoch [50/50], Loss: 2049.2855\n",
            "Mean Squared Error: 11798.6600\n",
            "Epoch [1/50], Loss: 2176.3471\n",
            "Epoch [2/50], Loss: 2157.7742\n",
            "Epoch [3/50], Loss: 2125.0648\n",
            "Epoch [4/50], Loss: 2096.1006\n",
            "Epoch [5/50], Loss: 2072.8997\n",
            "Epoch [6/50], Loss: 2058.0990\n",
            "Epoch [7/50], Loss: 2052.0172\n",
            "Epoch [8/50], Loss: 2048.6071\n",
            "Epoch [9/50], Loss: 2047.7781\n",
            "Epoch [10/50], Loss: 2050.8750\n",
            "Epoch [11/50], Loss: 2043.8817\n",
            "Epoch [12/50], Loss: 2042.8463\n",
            "Epoch [13/50], Loss: 2043.1947\n",
            "Epoch [14/50], Loss: 2042.2785\n",
            "Epoch [15/50], Loss: 2038.5171\n",
            "Epoch [16/50], Loss: 2041.3912\n",
            "Epoch [17/50], Loss: 2056.0149\n",
            "Epoch [18/50], Loss: 2044.3851\n",
            "Epoch [19/50], Loss: 2035.5932\n",
            "Epoch [20/50], Loss: 2040.8587\n",
            "Epoch [21/50], Loss: 2040.8997\n",
            "Epoch [22/50], Loss: 2031.5359\n",
            "Epoch [23/50], Loss: 2031.0442\n",
            "Epoch [24/50], Loss: 2030.9157\n",
            "Epoch [25/50], Loss: 2029.3994\n",
            "Epoch [26/50], Loss: 2026.2700\n",
            "Epoch [27/50], Loss: 2038.0113\n",
            "Epoch [28/50], Loss: 2032.6956\n",
            "Epoch [29/50], Loss: 2036.3794\n",
            "Epoch [30/50], Loss: 2024.5660\n",
            "Epoch [31/50], Loss: 2024.7123\n",
            "Epoch [32/50], Loss: 2025.6152\n",
            "Epoch [33/50], Loss: 2022.6001\n",
            "Epoch [34/50], Loss: 2031.4088\n",
            "Epoch [35/50], Loss: 2020.9989\n",
            "Epoch [36/50], Loss: 2019.8462\n",
            "Epoch [37/50], Loss: 2024.7988\n",
            "Epoch [38/50], Loss: 2018.2637\n",
            "Epoch [39/50], Loss: 2018.8424\n",
            "Epoch [40/50], Loss: 2018.7932\n",
            "Epoch [41/50], Loss: 2017.7131\n",
            "Epoch [42/50], Loss: 2015.7333\n",
            "Epoch [43/50], Loss: 2033.0620\n",
            "Epoch [44/50], Loss: 2021.5055\n",
            "Epoch [45/50], Loss: 2023.1589\n",
            "Epoch [46/50], Loss: 2040.6119\n",
            "Epoch [47/50], Loss: 2010.9179\n",
            "Epoch [48/50], Loss: 2012.0667\n",
            "Epoch [49/50], Loss: 2015.1499\n",
            "Epoch [50/50], Loss: 2018.6697\n",
            "Mean Squared Error: 11767.0734\n",
            "Epoch [1/50], Loss: 2181.0801\n",
            "Epoch [2/50], Loss: 2156.0339\n",
            "Epoch [3/50], Loss: 2091.7711\n",
            "Epoch [4/50], Loss: 2060.1355\n",
            "Epoch [5/50], Loss: 2068.3583\n",
            "Epoch [6/50], Loss: 2049.9820\n",
            "Epoch [7/50], Loss: 2056.1652\n",
            "Epoch [8/50], Loss: 2046.4566\n",
            "Epoch [9/50], Loss: 2046.9379\n",
            "Epoch [10/50], Loss: 2043.8116\n",
            "Epoch [11/50], Loss: 2041.8286\n",
            "Epoch [12/50], Loss: 2040.0570\n",
            "Epoch [13/50], Loss: 2174.8527\n",
            "Epoch [14/50], Loss: 2042.6705\n",
            "Epoch [15/50], Loss: 2173.1477\n",
            "Epoch [16/50], Loss: 2040.5208\n",
            "Epoch [17/50], Loss: 2053.2200\n",
            "Epoch [18/50], Loss: 2049.7857\n",
            "Epoch [19/50], Loss: 2032.8280\n",
            "Epoch [20/50], Loss: 2036.0224\n",
            "Epoch [21/50], Loss: 2031.0935\n",
            "Epoch [22/50], Loss: 2028.2948\n",
            "Epoch [23/50], Loss: 2028.8729\n",
            "Epoch [24/50], Loss: 2030.6285\n",
            "Epoch [25/50], Loss: 2036.5118\n",
            "Epoch [26/50], Loss: 2025.6081\n",
            "Epoch [27/50], Loss: 2024.9342\n",
            "Epoch [28/50], Loss: 2041.1924\n",
            "Epoch [29/50], Loss: 2026.6614\n",
            "Epoch [30/50], Loss: 2048.4510\n",
            "Epoch [31/50], Loss: 2039.1137\n",
            "Epoch [32/50], Loss: 2031.8909\n",
            "Epoch [33/50], Loss: 2026.1432\n",
            "Epoch [34/50], Loss: 2027.8951\n",
            "Epoch [35/50], Loss: 2020.8149\n",
            "Epoch [36/50], Loss: 2027.1154\n",
            "Epoch [37/50], Loss: 2021.1669\n",
            "Epoch [38/50], Loss: 2034.3747\n",
            "Epoch [39/50], Loss: 2025.3858\n",
            "Epoch [40/50], Loss: 2022.7991\n",
            "Epoch [41/50], Loss: 2016.4660\n",
            "Epoch [42/50], Loss: 2013.9981\n",
            "Epoch [43/50], Loss: 2022.0717\n",
            "Epoch [44/50], Loss: 2017.1573\n",
            "Epoch [45/50], Loss: 2011.5590\n",
            "Epoch [46/50], Loss: 2018.4568\n",
            "Epoch [47/50], Loss: 2016.9475\n",
            "Epoch [48/50], Loss: 2012.3174\n",
            "Epoch [49/50], Loss: 2008.8665\n",
            "Epoch [50/50], Loss: 2008.8771\n",
            "Mean Squared Error: 11775.5205\n",
            "Epoch [1/50], Loss: 2179.4139\n",
            "Epoch [2/50], Loss: 2118.4297\n",
            "Epoch [3/50], Loss: 2077.5202\n",
            "Epoch [4/50], Loss: 2052.9768\n",
            "Epoch [5/50], Loss: 2050.1842\n",
            "Epoch [6/50], Loss: 2050.8169\n",
            "Epoch [7/50], Loss: 2063.5223\n",
            "Epoch [8/50], Loss: 2053.3604\n",
            "Epoch [9/50], Loss: 2059.7671\n",
            "Epoch [10/50], Loss: 2039.5693\n",
            "Epoch [11/50], Loss: 2043.2102\n",
            "Epoch [12/50], Loss: 2035.6493\n",
            "Epoch [13/50], Loss: 2038.9198\n",
            "Epoch [14/50], Loss: 2053.4371\n",
            "Epoch [15/50], Loss: 2046.2818\n",
            "Epoch [16/50], Loss: 2029.7614\n",
            "Epoch [17/50], Loss: 2035.0207\n",
            "Epoch [18/50], Loss: 2034.2909\n",
            "Epoch [19/50], Loss: 2043.1930\n",
            "Epoch [20/50], Loss: 2030.8882\n",
            "Epoch [21/50], Loss: 2154.6261\n",
            "Epoch [22/50], Loss: 2149.7049\n",
            "Epoch [23/50], Loss: 2027.4791\n",
            "Epoch [24/50], Loss: 2154.4122\n",
            "Epoch [25/50], Loss: 2020.5959\n",
            "Epoch [26/50], Loss: 2016.0662\n",
            "Epoch [27/50], Loss: 2016.1733\n",
            "Epoch [28/50], Loss: 2015.4024\n",
            "Epoch [29/50], Loss: 2019.0312\n",
            "Epoch [30/50], Loss: 2018.1091\n",
            "Epoch [31/50], Loss: 2012.3927\n",
            "Epoch [32/50], Loss: 2024.7256\n",
            "Epoch [33/50], Loss: 2009.0601\n",
            "Epoch [34/50], Loss: 2006.7613\n",
            "Epoch [35/50], Loss: 2005.5677\n",
            "Epoch [36/50], Loss: 2002.9612\n",
            "Epoch [37/50], Loss: 2002.2270\n",
            "Epoch [38/50], Loss: 2002.4340\n",
            "Epoch [39/50], Loss: 1997.5760\n",
            "Epoch [40/50], Loss: 1999.7837\n",
            "Epoch [41/50], Loss: 2004.9757\n",
            "Epoch [42/50], Loss: 2008.3148\n",
            "Epoch [43/50], Loss: 2011.6681\n",
            "Epoch [44/50], Loss: 2011.1130\n",
            "Epoch [45/50], Loss: 2004.3475\n",
            "Epoch [46/50], Loss: 2018.5571\n",
            "Epoch [47/50], Loss: 2003.1479\n",
            "Epoch [48/50], Loss: 1999.9302\n",
            "Epoch [49/50], Loss: 1990.1571\n",
            "Epoch [50/50], Loss: 1996.1566\n",
            "Mean Squared Error: 11763.5363\n",
            "Epoch [1/50], Loss: 2179.1183\n",
            "Epoch [2/50], Loss: 2084.4487\n",
            "Epoch [3/50], Loss: 2054.0609\n",
            "Epoch [4/50], Loss: 2076.3487\n",
            "Epoch [5/50], Loss: 2053.9230\n",
            "Epoch [6/50], Loss: 2051.0031\n",
            "Epoch [7/50], Loss: 2053.5959\n",
            "Epoch [8/50], Loss: 2043.6606\n",
            "Epoch [9/50], Loss: 2044.2056\n",
            "Epoch [10/50], Loss: 2044.0032\n",
            "Epoch [11/50], Loss: 2036.9779\n",
            "Epoch [12/50], Loss: 2032.1191\n",
            "Epoch [13/50], Loss: 2031.3700\n",
            "Epoch [14/50], Loss: 2044.4546\n",
            "Epoch [15/50], Loss: 2032.3355\n",
            "Epoch [16/50], Loss: 2030.5365\n",
            "Epoch [17/50], Loss: 2030.2998\n",
            "Epoch [18/50], Loss: 2018.8564\n",
            "Epoch [19/50], Loss: 2017.8069\n",
            "Epoch [20/50], Loss: 2018.1403\n",
            "Epoch [21/50], Loss: 2167.6693\n",
            "Epoch [22/50], Loss: 2017.1804\n",
            "Epoch [23/50], Loss: 2020.8774\n",
            "Epoch [24/50], Loss: 2013.0300\n",
            "Epoch [25/50], Loss: 2008.9218\n",
            "Epoch [26/50], Loss: 2028.4978\n",
            "Epoch [27/50], Loss: 2004.5312\n",
            "Epoch [28/50], Loss: 1997.2130\n",
            "Epoch [29/50], Loss: 2009.9051\n",
            "Epoch [30/50], Loss: 1993.3343\n",
            "Epoch [31/50], Loss: 1990.7059\n",
            "Epoch [32/50], Loss: 1989.1601\n",
            "Epoch [33/50], Loss: 1989.3483\n",
            "Epoch [34/50], Loss: 1988.9096\n",
            "Epoch [35/50], Loss: 1983.0438\n",
            "Epoch [36/50], Loss: 1995.2156\n",
            "Epoch [37/50], Loss: 1984.4466\n",
            "Epoch [38/50], Loss: 1985.4854\n",
            "Epoch [39/50], Loss: 1976.2160\n",
            "Epoch [40/50], Loss: 1974.9828\n",
            "Epoch [41/50], Loss: 1968.4978\n",
            "Epoch [42/50], Loss: 1978.3202\n",
            "Epoch [43/50], Loss: 1966.6496\n",
            "Epoch [44/50], Loss: 1982.1751\n",
            "Epoch [45/50], Loss: 1982.4331\n",
            "Epoch [46/50], Loss: 1970.4040\n",
            "Epoch [47/50], Loss: 1977.7582\n",
            "Epoch [48/50], Loss: 1978.9206\n",
            "Epoch [49/50], Loss: 1982.8540\n",
            "Epoch [50/50], Loss: 1964.8360\n",
            "Mean Squared Error: 11709.2712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Activation Function Comparison\n",
        "results_activation = []\n",
        "for activation_name, activation_fn in activation_functions.items():\n",
        "    train_loader = create_data_loader(X_train_tensor, y_train_tensor, batch_size=32)\n",
        "    test_loader = create_data_loader(X_test_tensor, y_test_tensor, batch_size=32)\n",
        "\n",
        "    model = MLPRegressionModel(input_size, [16], activation_fn)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    mse = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=50)\n",
        "\n",
        "    results_activation.append({\n",
        "        'activation': activation_name,\n",
        "        'mse': mse\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0vLbyHvz3IJ",
        "outputId": "00758b74-67a1-4c8e-a77d-16af4150fabd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 2291.4622\n",
            "Epoch [2/50], Loss: 2097.4260\n",
            "Epoch [3/50], Loss: 2081.5678\n",
            "Epoch [4/50], Loss: 2055.7991\n",
            "Epoch [5/50], Loss: 2058.2922\n",
            "Epoch [6/50], Loss: 2056.4771\n",
            "Epoch [7/50], Loss: 2057.9917\n",
            "Epoch [8/50], Loss: 2045.2653\n",
            "Epoch [9/50], Loss: 2044.5350\n",
            "Epoch [10/50], Loss: 2195.0352\n",
            "Epoch [11/50], Loss: 2037.3543\n",
            "Epoch [12/50], Loss: 2052.0806\n",
            "Epoch [13/50], Loss: 2033.2155\n",
            "Epoch [14/50], Loss: 2032.9396\n",
            "Epoch [15/50], Loss: 2163.1381\n",
            "Epoch [16/50], Loss: 2038.3220\n",
            "Epoch [17/50], Loss: 2159.6360\n",
            "Epoch [18/50], Loss: 2033.1415\n",
            "Epoch [19/50], Loss: 2023.4155\n",
            "Epoch [20/50], Loss: 2037.8966\n",
            "Epoch [21/50], Loss: 2022.2406\n",
            "Epoch [22/50], Loss: 2031.1671\n",
            "Epoch [23/50], Loss: 2024.9735\n",
            "Epoch [24/50], Loss: 2019.3964\n",
            "Epoch [25/50], Loss: 2049.2796\n",
            "Epoch [26/50], Loss: 2018.7209\n",
            "Epoch [27/50], Loss: 2025.6196\n",
            "Epoch [28/50], Loss: 2015.7478\n",
            "Epoch [29/50], Loss: 2019.8346\n",
            "Epoch [30/50], Loss: 2016.9426\n",
            "Epoch [31/50], Loss: 2010.3800\n",
            "Epoch [32/50], Loss: 2013.6294\n",
            "Epoch [33/50], Loss: 2018.4856\n",
            "Epoch [34/50], Loss: 2012.4240\n",
            "Epoch [35/50], Loss: 2009.2543\n",
            "Epoch [36/50], Loss: 2009.7999\n",
            "Epoch [37/50], Loss: 2014.4198\n",
            "Epoch [38/50], Loss: 2006.0793\n",
            "Epoch [39/50], Loss: 2006.8843\n",
            "Epoch [40/50], Loss: 2014.5630\n",
            "Epoch [41/50], Loss: 2008.6249\n",
            "Epoch [42/50], Loss: 2010.4176\n",
            "Epoch [43/50], Loss: 2005.7464\n",
            "Epoch [44/50], Loss: 2006.4902\n",
            "Epoch [45/50], Loss: 2008.0744\n",
            "Epoch [46/50], Loss: 2143.8460\n",
            "Epoch [47/50], Loss: 2005.1501\n",
            "Epoch [48/50], Loss: 2017.6750\n",
            "Epoch [49/50], Loss: 2131.4053\n",
            "Epoch [50/50], Loss: 2002.5219\n",
            "Mean Squared Error: 11729.4914\n",
            "Epoch [1/50], Loss: 2192.3521\n",
            "Epoch [2/50], Loss: 2149.3696\n",
            "Epoch [3/50], Loss: 2136.5855\n",
            "Epoch [4/50], Loss: 2106.3377\n",
            "Epoch [5/50], Loss: 2095.5856\n",
            "Epoch [6/50], Loss: 2078.8986\n",
            "Epoch [7/50], Loss: 2079.0657\n",
            "Epoch [8/50], Loss: 2067.1073\n",
            "Epoch [9/50], Loss: 2197.3918\n",
            "Epoch [10/50], Loss: 2062.2920\n",
            "Epoch [11/50], Loss: 2062.7217\n",
            "Epoch [12/50], Loss: 2061.7955\n",
            "Epoch [13/50], Loss: 2059.1651\n",
            "Epoch [14/50], Loss: 2066.0307\n",
            "Epoch [15/50], Loss: 2060.0628\n",
            "Epoch [16/50], Loss: 2056.9522\n",
            "Epoch [17/50], Loss: 2053.6016\n",
            "Epoch [18/50], Loss: 2052.1673\n",
            "Epoch [19/50], Loss: 2062.0382\n",
            "Epoch [20/50], Loss: 2066.5746\n",
            "Epoch [21/50], Loss: 2052.5396\n",
            "Epoch [22/50], Loss: 2056.8493\n",
            "Epoch [23/50], Loss: 2056.0533\n",
            "Epoch [24/50], Loss: 2195.7215\n",
            "Epoch [25/50], Loss: 2063.7774\n",
            "Epoch [26/50], Loss: 2044.0295\n",
            "Epoch [27/50], Loss: 2043.1318\n",
            "Epoch [28/50], Loss: 2040.6518\n",
            "Epoch [29/50], Loss: 2047.9629\n",
            "Epoch [30/50], Loss: 2054.1051\n",
            "Epoch [31/50], Loss: 2038.5257\n",
            "Epoch [32/50], Loss: 2035.9923\n",
            "Epoch [33/50], Loss: 2042.6805\n",
            "Epoch [34/50], Loss: 2042.3687\n",
            "Epoch [35/50], Loss: 2045.2657\n",
            "Epoch [36/50], Loss: 2043.3172\n",
            "Epoch [37/50], Loss: 2047.0596\n",
            "Epoch [38/50], Loss: 2039.4903\n",
            "Epoch [39/50], Loss: 2047.4837\n",
            "Epoch [40/50], Loss: 2029.0057\n",
            "Epoch [41/50], Loss: 2027.1901\n",
            "Epoch [42/50], Loss: 2026.4896\n",
            "Epoch [43/50], Loss: 2029.3951\n",
            "Epoch [44/50], Loss: 2026.3954\n",
            "Epoch [45/50], Loss: 2164.4604\n",
            "Epoch [46/50], Loss: 2023.9182\n",
            "Epoch [47/50], Loss: 2026.0146\n",
            "Epoch [48/50], Loss: 2153.6025\n",
            "Epoch [49/50], Loss: 2021.2048\n",
            "Epoch [50/50], Loss: 2021.2723\n",
            "Mean Squared Error: 11731.3524\n",
            "Epoch [1/50], Loss: 2167.7861\n",
            "Epoch [2/50], Loss: 2116.4748\n",
            "Epoch [3/50], Loss: 2079.0077\n",
            "Epoch [4/50], Loss: 2051.2280\n",
            "Epoch [5/50], Loss: 2049.5974\n",
            "Epoch [6/50], Loss: 2048.6709\n",
            "Epoch [7/50], Loss: 2044.8785\n",
            "Epoch [8/50], Loss: 2065.6644\n",
            "Epoch [9/50], Loss: 2197.4517\n",
            "Epoch [10/50], Loss: 2048.8413\n",
            "Epoch [11/50], Loss: 2041.6425\n",
            "Epoch [12/50], Loss: 2042.6609\n",
            "Epoch [13/50], Loss: 2045.6111\n",
            "Epoch [14/50], Loss: 2052.0760\n",
            "Epoch [15/50], Loss: 2038.6554\n",
            "Epoch [16/50], Loss: 2037.8596\n",
            "Epoch [17/50], Loss: 2035.7353\n",
            "Epoch [18/50], Loss: 2028.7342\n",
            "Epoch [19/50], Loss: 2029.9612\n",
            "Epoch [20/50], Loss: 2044.6060\n",
            "Epoch [21/50], Loss: 2040.4718\n",
            "Epoch [22/50], Loss: 2025.0101\n",
            "Epoch [23/50], Loss: 2152.9743\n",
            "Epoch [24/50], Loss: 2023.6345\n",
            "Epoch [25/50], Loss: 2160.5716\n",
            "Epoch [26/50], Loss: 2021.1641\n",
            "Epoch [27/50], Loss: 2025.4531\n",
            "Epoch [28/50], Loss: 2032.4620\n",
            "Epoch [29/50], Loss: 2017.7384\n",
            "Epoch [30/50], Loss: 2143.5048\n",
            "Epoch [31/50], Loss: 2014.3395\n",
            "Epoch [32/50], Loss: 2014.6913\n",
            "Epoch [33/50], Loss: 2009.8733\n",
            "Epoch [34/50], Loss: 2010.2198\n",
            "Epoch [35/50], Loss: 2016.6714\n",
            "Epoch [36/50], Loss: 2014.2263\n",
            "Epoch [37/50], Loss: 2008.1575\n",
            "Epoch [38/50], Loss: 2018.8097\n",
            "Epoch [39/50], Loss: 2011.5437\n",
            "Epoch [40/50], Loss: 2010.2089\n",
            "Epoch [41/50], Loss: 2137.0380\n",
            "Epoch [42/50], Loss: 2002.3277\n",
            "Epoch [43/50], Loss: 2013.8563\n",
            "Epoch [44/50], Loss: 2013.1268\n",
            "Epoch [45/50], Loss: 2013.7060\n",
            "Epoch [46/50], Loss: 2013.2580\n",
            "Epoch [47/50], Loss: 2002.4903\n",
            "Epoch [48/50], Loss: 2006.1846\n",
            "Epoch [49/50], Loss: 1997.0026\n",
            "Epoch [50/50], Loss: 2004.3135\n",
            "Mean Squared Error: 11732.4010\n",
            "Epoch [1/50], Loss: 2190.0868\n",
            "Epoch [2/50], Loss: 2182.6586\n",
            "Epoch [3/50], Loss: 2179.8735\n",
            "Epoch [4/50], Loss: 2172.9988\n",
            "Epoch [5/50], Loss: 2169.3238\n",
            "Epoch [6/50], Loss: 2171.0924\n",
            "Epoch [7/50], Loss: 2163.6898\n",
            "Epoch [8/50], Loss: 2153.9129\n",
            "Epoch [9/50], Loss: 2154.8514\n",
            "Epoch [10/50], Loss: 2280.0923\n",
            "Epoch [11/50], Loss: 2137.6615\n",
            "Epoch [12/50], Loss: 2152.1482\n",
            "Epoch [13/50], Loss: 2137.6928\n",
            "Epoch [14/50], Loss: 2126.6287\n",
            "Epoch [15/50], Loss: 2130.9209\n",
            "Epoch [16/50], Loss: 2270.8141\n",
            "Epoch [17/50], Loss: 2119.1569\n",
            "Epoch [18/50], Loss: 2115.5093\n",
            "Epoch [19/50], Loss: 2114.3069\n",
            "Epoch [20/50], Loss: 2111.8819\n",
            "Epoch [21/50], Loss: 2108.3757\n",
            "Epoch [22/50], Loss: 2106.1067\n",
            "Epoch [23/50], Loss: 2121.2514\n",
            "Epoch [24/50], Loss: 2104.6951\n",
            "Epoch [25/50], Loss: 2100.2691\n",
            "Epoch [26/50], Loss: 2098.4839\n",
            "Epoch [27/50], Loss: 2110.6452\n",
            "Epoch [28/50], Loss: 2095.2705\n",
            "Epoch [29/50], Loss: 2096.6496\n",
            "Epoch [30/50], Loss: 2094.1165\n",
            "Epoch [31/50], Loss: 2093.0087\n",
            "Epoch [32/50], Loss: 2096.8481\n",
            "Epoch [33/50], Loss: 2251.8749\n",
            "Epoch [34/50], Loss: 2092.0046\n",
            "Epoch [35/50], Loss: 2085.0284\n",
            "Epoch [36/50], Loss: 2083.6841\n",
            "Epoch [37/50], Loss: 2082.8193\n",
            "Epoch [38/50], Loss: 2092.5821\n",
            "Epoch [39/50], Loss: 2080.9535\n",
            "Epoch [40/50], Loss: 2080.8741\n",
            "Epoch [41/50], Loss: 2101.4831\n",
            "Epoch [42/50], Loss: 2077.0626\n",
            "Epoch [43/50], Loss: 2075.8991\n",
            "Epoch [44/50], Loss: 2084.3462\n",
            "Epoch [45/50], Loss: 2073.9387\n",
            "Epoch [46/50], Loss: 2072.8957\n",
            "Epoch [47/50], Loss: 2072.0391\n",
            "Epoch [48/50], Loss: 2072.1910\n",
            "Epoch [49/50], Loss: 2070.1399\n",
            "Epoch [50/50], Loss: 2087.5414\n",
            "Mean Squared Error: 11916.5324\n",
            "Epoch [1/50], Loss: 2181.5306\n",
            "Epoch [2/50], Loss: 2143.9642\n",
            "Epoch [3/50], Loss: 2114.3873\n",
            "Epoch [4/50], Loss: 2085.6805\n",
            "Epoch [5/50], Loss: 2082.2146\n",
            "Epoch [6/50], Loss: 2070.3903\n",
            "Epoch [7/50], Loss: 2065.1778\n",
            "Epoch [8/50], Loss: 2071.6019\n",
            "Epoch [9/50], Loss: 2071.4068\n",
            "Epoch [10/50], Loss: 2059.9200\n",
            "Epoch [11/50], Loss: 2058.0288\n",
            "Epoch [12/50], Loss: 2066.5576\n",
            "Epoch [13/50], Loss: 2055.8613\n",
            "Epoch [14/50], Loss: 2053.2590\n",
            "Epoch [15/50], Loss: 2059.0298\n",
            "Epoch [16/50], Loss: 2064.7778\n",
            "Epoch [17/50], Loss: 2053.5504\n",
            "Epoch [18/50], Loss: 2052.2120\n",
            "Epoch [19/50], Loss: 2041.6862\n",
            "Epoch [20/50], Loss: 2047.0405\n",
            "Epoch [21/50], Loss: 2042.0173\n",
            "Epoch [22/50], Loss: 2041.8649\n",
            "Epoch [23/50], Loss: 2035.4682\n",
            "Epoch [24/50], Loss: 2030.5220\n",
            "Epoch [25/50], Loss: 2027.1171\n",
            "Epoch [26/50], Loss: 2026.8728\n",
            "Epoch [27/50], Loss: 2021.7913\n",
            "Epoch [28/50], Loss: 2026.2257\n",
            "Epoch [29/50], Loss: 2026.7202\n",
            "Epoch [30/50], Loss: 2021.7740\n",
            "Epoch [31/50], Loss: 2017.5892\n",
            "Epoch [32/50], Loss: 2018.7416\n",
            "Epoch [33/50], Loss: 2027.5097\n",
            "Epoch [34/50], Loss: 2010.0947\n",
            "Epoch [35/50], Loss: 2011.6218\n",
            "Epoch [36/50], Loss: 2006.0554\n",
            "Epoch [37/50], Loss: 2006.6262\n",
            "Epoch [38/50], Loss: 2006.3211\n",
            "Epoch [39/50], Loss: 2020.1645\n",
            "Epoch [40/50], Loss: 2002.5227\n",
            "Epoch [41/50], Loss: 2003.2952\n",
            "Epoch [42/50], Loss: 1996.0902\n",
            "Epoch [43/50], Loss: 1990.3482\n",
            "Epoch [44/50], Loss: 1989.2097\n",
            "Epoch [45/50], Loss: 1986.1501\n",
            "Epoch [46/50], Loss: 1988.4898\n",
            "Epoch [47/50], Loss: 1985.2534\n",
            "Epoch [48/50], Loss: 1979.8703\n",
            "Epoch [49/50], Loss: 1981.7013\n",
            "Epoch [50/50], Loss: 1993.3046\n",
            "Mean Squared Error: 11672.8370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Learning Rate Comparison\n",
        "results_learning_rate = []\n",
        "for lr in learning_rates:\n",
        "    train_loader = create_data_loader(X_train_tensor, y_train_tensor, batch_size=32)\n",
        "    test_loader = create_data_loader(X_test_tensor, y_test_tensor, batch_size=32)\n",
        "\n",
        "    model = MLPRegressionModel(input_size, [16], nn.ReLU())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    mse = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=50)\n",
        "\n",
        "    results_learning_rate.append({\n",
        "        'learning_rate': lr,\n",
        "        'mse': mse\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbOLRwAVz3-n",
        "outputId": "40940c00-2bc8-4176-8a16-f08592964bd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 2311936.5222\n",
            "Epoch [2/50], Loss: 4187.8858\n",
            "Epoch [3/50], Loss: 4302.1681\n",
            "Epoch [4/50], Loss: 3863.6001\n",
            "Epoch [5/50], Loss: 3344.7958\n",
            "Epoch [6/50], Loss: 2887.8004\n",
            "Epoch [7/50], Loss: 2576.2280\n",
            "Epoch [8/50], Loss: 2349.0792\n",
            "Epoch [9/50], Loss: 2215.6993\n",
            "Epoch [10/50], Loss: 2142.6526\n",
            "Epoch [11/50], Loss: 2111.0075\n",
            "Epoch [12/50], Loss: 2095.3467\n",
            "Epoch [13/50], Loss: 2076.6900\n",
            "Epoch [14/50], Loss: 2067.7410\n",
            "Epoch [15/50], Loss: 2067.5886\n",
            "Epoch [16/50], Loss: 2071.0108\n",
            "Epoch [17/50], Loss: 2079.3201\n",
            "Epoch [18/50], Loss: 2201.2668\n",
            "Epoch [19/50], Loss: 2086.1123\n",
            "Epoch [20/50], Loss: 2066.9348\n",
            "Epoch [21/50], Loss: 2066.2945\n",
            "Epoch [22/50], Loss: 2067.2308\n",
            "Epoch [23/50], Loss: 2091.9310\n",
            "Epoch [24/50], Loss: 2066.4935\n",
            "Epoch [25/50], Loss: 2066.2342\n",
            "Epoch [26/50], Loss: 2066.8037\n",
            "Epoch [27/50], Loss: 2201.8015\n",
            "Epoch [28/50], Loss: 2066.8842\n",
            "Epoch [29/50], Loss: 2067.1526\n",
            "Epoch [30/50], Loss: 2081.8118\n",
            "Epoch [31/50], Loss: 2071.5023\n",
            "Epoch [32/50], Loss: 2071.5877\n",
            "Epoch [33/50], Loss: 2066.3957\n",
            "Epoch [34/50], Loss: 2066.2787\n",
            "Epoch [35/50], Loss: 2069.1805\n",
            "Epoch [36/50], Loss: 2075.7922\n",
            "Epoch [37/50], Loss: 2068.0312\n",
            "Epoch [38/50], Loss: 2067.8385\n",
            "Epoch [39/50], Loss: 2076.5043\n",
            "Epoch [40/50], Loss: 2074.7665\n",
            "Epoch [41/50], Loss: 2072.2502\n",
            "Epoch [42/50], Loss: 2066.2331\n",
            "Epoch [43/50], Loss: 2085.1273\n",
            "Epoch [44/50], Loss: 2067.9324\n",
            "Epoch [45/50], Loss: 2069.0433\n",
            "Epoch [46/50], Loss: 2065.5817\n",
            "Epoch [47/50], Loss: 2073.2413\n",
            "Epoch [48/50], Loss: 2066.0983\n",
            "Epoch [49/50], Loss: 2075.4818\n",
            "Epoch [50/50], Loss: 2098.6045\n",
            "Mean Squared Error: 11864.6650\n",
            "Epoch [1/50], Loss: 2500.0475\n",
            "Epoch [2/50], Loss: 2123.1450\n",
            "Epoch [3/50], Loss: 2072.3179\n",
            "Epoch [4/50], Loss: 2075.2236\n",
            "Epoch [5/50], Loss: 2070.3541\n",
            "Epoch [6/50], Loss: 2071.5148\n",
            "Epoch [7/50], Loss: 2066.5725\n",
            "Epoch [8/50], Loss: 2092.2904\n",
            "Epoch [9/50], Loss: 2066.5653\n",
            "Epoch [10/50], Loss: 2076.6394\n",
            "Epoch [11/50], Loss: 2068.7251\n",
            "Epoch [12/50], Loss: 2071.6541\n",
            "Epoch [13/50], Loss: 2069.6351\n",
            "Epoch [14/50], Loss: 2068.2290\n",
            "Epoch [15/50], Loss: 2067.7262\n",
            "Epoch [16/50], Loss: 2071.7145\n",
            "Epoch [17/50], Loss: 2067.8764\n",
            "Epoch [18/50], Loss: 2067.2979\n",
            "Epoch [19/50], Loss: 2072.7121\n",
            "Epoch [20/50], Loss: 2075.9703\n",
            "Epoch [21/50], Loss: 2090.0970\n",
            "Epoch [22/50], Loss: 2079.4090\n",
            "Epoch [23/50], Loss: 2072.0491\n",
            "Epoch [24/50], Loss: 2067.5758\n",
            "Epoch [25/50], Loss: 2066.9564\n",
            "Epoch [26/50], Loss: 2067.3102\n",
            "Epoch [27/50], Loss: 2076.8063\n",
            "Epoch [28/50], Loss: 2075.2607\n",
            "Epoch [29/50], Loss: 2066.6217\n",
            "Epoch [30/50], Loss: 2067.0755\n",
            "Epoch [31/50], Loss: 2068.3602\n",
            "Epoch [32/50], Loss: 2067.7865\n",
            "Epoch [33/50], Loss: 2076.1227\n",
            "Epoch [34/50], Loss: 2223.4708\n",
            "Epoch [35/50], Loss: 2073.8886\n",
            "Epoch [36/50], Loss: 2074.1155\n",
            "Epoch [37/50], Loss: 2071.2792\n",
            "Epoch [38/50], Loss: 2069.6999\n",
            "Epoch [39/50], Loss: 2071.9098\n",
            "Epoch [40/50], Loss: 2068.3970\n",
            "Epoch [41/50], Loss: 2067.4410\n",
            "Epoch [42/50], Loss: 2067.5804\n",
            "Epoch [43/50], Loss: 2067.8736\n",
            "Epoch [44/50], Loss: 2070.6491\n",
            "Epoch [45/50], Loss: 2075.4421\n",
            "Epoch [46/50], Loss: 2089.1133\n",
            "Epoch [47/50], Loss: 2232.7928\n",
            "Epoch [48/50], Loss: 2074.2319\n",
            "Epoch [49/50], Loss: 2079.2968\n",
            "Epoch [50/50], Loss: 2077.5341\n",
            "Mean Squared Error: 11853.0444\n",
            "Epoch [1/50], Loss: 2071.3562\n",
            "Epoch [2/50], Loss: 2056.0185\n",
            "Epoch [3/50], Loss: 2044.9640\n",
            "Epoch [4/50], Loss: 2069.7149\n",
            "Epoch [5/50], Loss: 2035.8102\n",
            "Epoch [6/50], Loss: 2044.9348\n",
            "Epoch [7/50], Loss: 2020.3439\n",
            "Epoch [8/50], Loss: 2031.9631\n",
            "Epoch [9/50], Loss: 2019.9447\n",
            "Epoch [10/50], Loss: 2015.6372\n",
            "Epoch [11/50], Loss: 2029.2148\n",
            "Epoch [12/50], Loss: 2011.4073\n",
            "Epoch [13/50], Loss: 2004.2026\n",
            "Epoch [14/50], Loss: 2001.2085\n",
            "Epoch [15/50], Loss: 2119.9949\n",
            "Epoch [16/50], Loss: 2010.0373\n",
            "Epoch [17/50], Loss: 1993.5647\n",
            "Epoch [18/50], Loss: 1962.4044\n",
            "Epoch [19/50], Loss: 1977.7852\n",
            "Epoch [20/50], Loss: 1995.5266\n",
            "Epoch [21/50], Loss: 1954.3909\n",
            "Epoch [22/50], Loss: 1966.5928\n",
            "Epoch [23/50], Loss: 1932.8053\n",
            "Epoch [24/50], Loss: 1936.9692\n",
            "Epoch [25/50], Loss: 1909.0333\n",
            "Epoch [26/50], Loss: 2062.3108\n",
            "Epoch [27/50], Loss: 1870.0267\n",
            "Epoch [28/50], Loss: 1914.6955\n",
            "Epoch [29/50], Loss: 1934.2962\n",
            "Epoch [30/50], Loss: 2078.3408\n",
            "Epoch [31/50], Loss: 1919.9451\n",
            "Epoch [32/50], Loss: 1903.6061\n",
            "Epoch [33/50], Loss: 1940.2495\n",
            "Epoch [34/50], Loss: 1896.5152\n",
            "Epoch [35/50], Loss: 1821.5291\n",
            "Epoch [36/50], Loss: 1836.2603\n",
            "Epoch [37/50], Loss: 1839.0620\n",
            "Epoch [38/50], Loss: 1823.1897\n",
            "Epoch [39/50], Loss: 1829.6023\n",
            "Epoch [40/50], Loss: 1881.6852\n",
            "Epoch [41/50], Loss: 1789.7973\n",
            "Epoch [42/50], Loss: 1797.5702\n",
            "Epoch [43/50], Loss: 1783.3065\n",
            "Epoch [44/50], Loss: 1790.2548\n",
            "Epoch [45/50], Loss: 1784.1561\n",
            "Epoch [46/50], Loss: 1769.5561\n",
            "Epoch [47/50], Loss: 1734.0113\n",
            "Epoch [48/50], Loss: 1728.1679\n",
            "Epoch [49/50], Loss: 1773.1207\n",
            "Epoch [50/50], Loss: 1698.7043\n",
            "Mean Squared Error: 11621.9927\n",
            "Epoch [1/50], Loss: 2180.7239\n",
            "Epoch [2/50], Loss: 2147.2401\n",
            "Epoch [3/50], Loss: 2112.9314\n",
            "Epoch [4/50], Loss: 2072.3610\n",
            "Epoch [5/50], Loss: 2081.6238\n",
            "Epoch [6/50], Loss: 2070.5893\n",
            "Epoch [7/50], Loss: 2050.0419\n",
            "Epoch [8/50], Loss: 2047.7691\n",
            "Epoch [9/50], Loss: 2052.4693\n",
            "Epoch [10/50], Loss: 2063.1323\n",
            "Epoch [11/50], Loss: 2061.4459\n",
            "Epoch [12/50], Loss: 2045.1765\n",
            "Epoch [13/50], Loss: 2042.9395\n",
            "Epoch [14/50], Loss: 2040.3846\n",
            "Epoch [15/50], Loss: 2038.1422\n",
            "Epoch [16/50], Loss: 2046.9251\n",
            "Epoch [17/50], Loss: 2039.4702\n",
            "Epoch [18/50], Loss: 2035.8523\n",
            "Epoch [19/50], Loss: 2040.1254\n",
            "Epoch [20/50], Loss: 2040.5815\n",
            "Epoch [21/50], Loss: 2169.0217\n",
            "Epoch [22/50], Loss: 2031.3048\n",
            "Epoch [23/50], Loss: 2032.9690\n",
            "Epoch [24/50], Loss: 2036.7951\n",
            "Epoch [25/50], Loss: 2030.2336\n",
            "Epoch [26/50], Loss: 2030.8362\n",
            "Epoch [27/50], Loss: 2025.8631\n",
            "Epoch [28/50], Loss: 2028.8824\n",
            "Epoch [29/50], Loss: 2041.6043\n",
            "Epoch [30/50], Loss: 2025.6735\n",
            "Epoch [31/50], Loss: 2024.7175\n",
            "Epoch [32/50], Loss: 2023.9638\n",
            "Epoch [33/50], Loss: 2032.3764\n",
            "Epoch [34/50], Loss: 2026.3008\n",
            "Epoch [35/50], Loss: 2038.0743\n",
            "Epoch [36/50], Loss: 2043.1282\n",
            "Epoch [37/50], Loss: 2019.5630\n",
            "Epoch [38/50], Loss: 2028.6161\n",
            "Epoch [39/50], Loss: 2049.6256\n",
            "Epoch [40/50], Loss: 2020.0912\n",
            "Epoch [41/50], Loss: 2017.3611\n",
            "Epoch [42/50], Loss: 2023.0933\n",
            "Epoch [43/50], Loss: 2043.5265\n",
            "Epoch [44/50], Loss: 2014.9155\n",
            "Epoch [45/50], Loss: 2018.5229\n",
            "Epoch [46/50], Loss: 2015.6070\n",
            "Epoch [47/50], Loss: 2020.2424\n",
            "Epoch [48/50], Loss: 2144.2255\n",
            "Epoch [49/50], Loss: 2016.0072\n",
            "Epoch [50/50], Loss: 2016.0476\n",
            "Mean Squared Error: 11783.8653\n",
            "Epoch [1/50], Loss: 2195.4598\n",
            "Epoch [2/50], Loss: 2192.2321\n",
            "Epoch [3/50], Loss: 2189.5884\n",
            "Epoch [4/50], Loss: 2186.0924\n",
            "Epoch [5/50], Loss: 2197.6943\n",
            "Epoch [6/50], Loss: 2180.4621\n",
            "Epoch [7/50], Loss: 2179.4326\n",
            "Epoch [8/50], Loss: 2175.8890\n",
            "Epoch [9/50], Loss: 2170.0288\n",
            "Epoch [10/50], Loss: 2164.5685\n",
            "Epoch [11/50], Loss: 2179.8101\n",
            "Epoch [12/50], Loss: 2156.2818\n",
            "Epoch [13/50], Loss: 2166.0610\n",
            "Epoch [14/50], Loss: 2142.6373\n",
            "Epoch [15/50], Loss: 2136.5757\n",
            "Epoch [16/50], Loss: 2137.7764\n",
            "Epoch [17/50], Loss: 2141.1150\n",
            "Epoch [18/50], Loss: 2137.5169\n",
            "Epoch [19/50], Loss: 2267.6250\n",
            "Epoch [20/50], Loss: 2108.0350\n",
            "Epoch [21/50], Loss: 2113.0333\n",
            "Epoch [22/50], Loss: 2096.7476\n",
            "Epoch [23/50], Loss: 2092.0915\n",
            "Epoch [24/50], Loss: 2095.1345\n",
            "Epoch [25/50], Loss: 2090.9136\n",
            "Epoch [26/50], Loss: 2078.3440\n",
            "Epoch [27/50], Loss: 2084.5566\n",
            "Epoch [28/50], Loss: 2071.9656\n",
            "Epoch [29/50], Loss: 2206.1884\n",
            "Epoch [30/50], Loss: 2068.9547\n",
            "Epoch [31/50], Loss: 2065.4054\n",
            "Epoch [32/50], Loss: 2062.0145\n",
            "Epoch [33/50], Loss: 2060.7020\n",
            "Epoch [34/50], Loss: 2059.7226\n",
            "Epoch [35/50], Loss: 2062.1939\n",
            "Epoch [36/50], Loss: 2060.8329\n",
            "Epoch [37/50], Loss: 2058.9661\n",
            "Epoch [38/50], Loss: 2068.1273\n",
            "Epoch [39/50], Loss: 2061.2609\n",
            "Epoch [40/50], Loss: 2054.0894\n",
            "Epoch [41/50], Loss: 2052.3324\n",
            "Epoch [42/50], Loss: 2059.8098\n",
            "Epoch [43/50], Loss: 2053.2833\n",
            "Epoch [44/50], Loss: 2051.5953\n",
            "Epoch [45/50], Loss: 2049.9776\n",
            "Epoch [46/50], Loss: 2058.5759\n",
            "Epoch [47/50], Loss: 2057.4488\n",
            "Epoch [48/50], Loss: 2190.2625\n",
            "Epoch [49/50], Loss: 2182.4461\n",
            "Epoch [50/50], Loss: 2048.6116\n",
            "Mean Squared Error: 11829.2249\n",
            "Epoch [1/50], Loss: 2199.3419\n",
            "Epoch [2/50], Loss: 2190.8334\n",
            "Epoch [3/50], Loss: 2192.2993\n",
            "Epoch [4/50], Loss: 2191.0482\n",
            "Epoch [5/50], Loss: 2329.4153\n",
            "Epoch [6/50], Loss: 2192.3747\n",
            "Epoch [7/50], Loss: 2329.1939\n",
            "Epoch [8/50], Loss: 2190.8882\n",
            "Epoch [9/50], Loss: 2190.2185\n",
            "Epoch [10/50], Loss: 2192.4259\n",
            "Epoch [11/50], Loss: 2191.6007\n",
            "Epoch [12/50], Loss: 2215.8748\n",
            "Epoch [13/50], Loss: 2196.2845\n",
            "Epoch [14/50], Loss: 2191.5587\n",
            "Epoch [15/50], Loss: 2189.3106\n",
            "Epoch [16/50], Loss: 2190.5823\n",
            "Epoch [17/50], Loss: 2187.7022\n",
            "Epoch [18/50], Loss: 2195.5741\n",
            "Epoch [19/50], Loss: 2186.5789\n",
            "Epoch [20/50], Loss: 2204.0867\n",
            "Epoch [21/50], Loss: 2188.2352\n",
            "Epoch [22/50], Loss: 2186.9476\n",
            "Epoch [23/50], Loss: 2185.5975\n",
            "Epoch [24/50], Loss: 2186.5533\n",
            "Epoch [25/50], Loss: 2186.6176\n",
            "Epoch [26/50], Loss: 2190.4104\n",
            "Epoch [27/50], Loss: 2184.1401\n",
            "Epoch [28/50], Loss: 2185.6139\n",
            "Epoch [29/50], Loss: 2194.6528\n",
            "Epoch [30/50], Loss: 2191.4259\n",
            "Epoch [31/50], Loss: 2195.3087\n",
            "Epoch [32/50], Loss: 2202.0017\n",
            "Epoch [33/50], Loss: 2182.7201\n",
            "Epoch [34/50], Loss: 2191.9184\n",
            "Epoch [35/50], Loss: 2182.8448\n",
            "Epoch [36/50], Loss: 2182.2066\n",
            "Epoch [37/50], Loss: 2181.7771\n",
            "Epoch [38/50], Loss: 2181.1696\n",
            "Epoch [39/50], Loss: 2193.3835\n",
            "Epoch [40/50], Loss: 2185.9552\n",
            "Epoch [41/50], Loss: 2318.4148\n",
            "Epoch [42/50], Loss: 2186.5009\n",
            "Epoch [43/50], Loss: 2179.7877\n",
            "Epoch [44/50], Loss: 2320.4735\n",
            "Epoch [45/50], Loss: 2195.5730\n",
            "Epoch [46/50], Loss: 2177.6258\n",
            "Epoch [47/50], Loss: 2174.8996\n",
            "Epoch [48/50], Loss: 2174.5751\n",
            "Epoch [49/50], Loss: 2174.2254\n",
            "Epoch [50/50], Loss: 2173.9839\n",
            "Mean Squared Error: 12144.7805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Batch Size Comparison\n",
        "results_batch_size = []\n",
        "for batch_size in batch_sizes:\n",
        "    train_loader = create_data_loader(X_train_tensor, y_train_tensor, batch_size=batch_size)\n",
        "    test_loader = create_data_loader(X_test_tensor, y_test_tensor, batch_size=batch_size)\n",
        "\n",
        "    model = MLPRegressionModel(input_size, [16], nn.ReLU())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    mse = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=50)\n",
        "\n",
        "    results_batch_size.append({\n",
        "        'batch_size': batch_size,\n",
        "        'mse': mse\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmWJiHZjz6Mm",
        "outputId": "7b76d5e3-35e5-4653-9673-08bf9d9ebc87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 2148.3608\n",
            "Epoch [2/50], Loss: 2107.9127\n",
            "Epoch [3/50], Loss: 2056.5546\n",
            "Epoch [4/50], Loss: 2055.0482\n",
            "Epoch [5/50], Loss: 2052.2893\n",
            "Epoch [6/50], Loss: 2048.1825\n",
            "Epoch [7/50], Loss: 2343.8899\n",
            "Epoch [8/50], Loss: 2047.5640\n",
            "Epoch [9/50], Loss: 2079.7422\n",
            "Epoch [10/50], Loss: 2041.0874\n",
            "Epoch [11/50], Loss: 2043.7129\n",
            "Epoch [12/50], Loss: 2042.5846\n",
            "Epoch [13/50], Loss: 2039.5190\n",
            "Epoch [14/50], Loss: 2035.6737\n",
            "Epoch [15/50], Loss: 2033.9278\n",
            "Epoch [16/50], Loss: 2033.2625\n",
            "Epoch [17/50], Loss: 2034.8443\n",
            "Epoch [18/50], Loss: 2028.4874\n",
            "Epoch [19/50], Loss: 2026.9895\n",
            "Epoch [20/50], Loss: 2027.6486\n",
            "Epoch [21/50], Loss: 2319.3294\n",
            "Epoch [22/50], Loss: 2024.8414\n",
            "Epoch [23/50], Loss: 2033.2171\n",
            "Epoch [24/50], Loss: 2030.3977\n",
            "Epoch [25/50], Loss: 2039.1982\n",
            "Epoch [26/50], Loss: 2019.7172\n",
            "Epoch [27/50], Loss: 2024.8045\n",
            "Epoch [28/50], Loss: 2017.1033\n",
            "Epoch [29/50], Loss: 2058.3851\n",
            "Epoch [30/50], Loss: 2016.7563\n",
            "Epoch [31/50], Loss: 2015.9466\n",
            "Epoch [32/50], Loss: 2014.7347\n",
            "Epoch [33/50], Loss: 2015.0676\n",
            "Epoch [34/50], Loss: 2011.5471\n",
            "Epoch [35/50], Loss: 2026.0446\n",
            "Epoch [36/50], Loss: 2010.4272\n",
            "Epoch [37/50], Loss: 2014.6784\n",
            "Epoch [38/50], Loss: 2017.6059\n",
            "Epoch [39/50], Loss: 2008.6672\n",
            "Epoch [40/50], Loss: 2006.9386\n",
            "Epoch [41/50], Loss: 2300.6207\n",
            "Epoch [42/50], Loss: 2009.5863\n",
            "Epoch [43/50], Loss: 2015.4294\n",
            "Epoch [44/50], Loss: 2005.5932\n",
            "Epoch [45/50], Loss: 2024.6303\n",
            "Epoch [46/50], Loss: 2005.0733\n",
            "Epoch [47/50], Loss: 2005.6747\n",
            "Epoch [48/50], Loss: 2003.6866\n",
            "Epoch [49/50], Loss: 2001.6815\n",
            "Epoch [50/50], Loss: 2001.1645\n",
            "Mean Squared Error: 11729.9559\n",
            "Epoch [1/50], Loss: 2181.6560\n",
            "Epoch [2/50], Loss: 2152.3130\n",
            "Epoch [3/50], Loss: 2108.2880\n",
            "Epoch [4/50], Loss: 2087.0100\n",
            "Epoch [5/50], Loss: 2057.1226\n",
            "Epoch [6/50], Loss: 2048.6095\n",
            "Epoch [7/50], Loss: 2182.8756\n",
            "Epoch [8/50], Loss: 2064.8251\n",
            "Epoch [9/50], Loss: 2062.5212\n",
            "Epoch [10/50], Loss: 2041.9052\n",
            "Epoch [11/50], Loss: 2048.1516\n",
            "Epoch [12/50], Loss: 2047.2485\n",
            "Epoch [13/50], Loss: 2037.8402\n",
            "Epoch [14/50], Loss: 2035.1397\n",
            "Epoch [15/50], Loss: 2034.4348\n",
            "Epoch [16/50], Loss: 2035.7853\n",
            "Epoch [17/50], Loss: 2044.0676\n",
            "Epoch [18/50], Loss: 2036.3505\n",
            "Epoch [19/50], Loss: 2171.6776\n",
            "Epoch [20/50], Loss: 2030.9491\n",
            "Epoch [21/50], Loss: 2028.5971\n",
            "Epoch [22/50], Loss: 2031.8790\n",
            "Epoch [23/50], Loss: 2026.7855\n",
            "Epoch [24/50], Loss: 2032.0919\n",
            "Epoch [25/50], Loss: 2031.4533\n",
            "Epoch [26/50], Loss: 2157.8921\n",
            "Epoch [27/50], Loss: 2035.3065\n",
            "Epoch [28/50], Loss: 2020.0127\n",
            "Epoch [29/50], Loss: 2022.1909\n",
            "Epoch [30/50], Loss: 2018.2173\n",
            "Epoch [31/50], Loss: 2020.2563\n",
            "Epoch [32/50], Loss: 2021.4873\n",
            "Epoch [33/50], Loss: 2035.7415\n",
            "Epoch [34/50], Loss: 2016.7535\n",
            "Epoch [35/50], Loss: 2015.8318\n",
            "Epoch [36/50], Loss: 2020.3636\n",
            "Epoch [37/50], Loss: 2013.6878\n",
            "Epoch [38/50], Loss: 2015.3221\n",
            "Epoch [39/50], Loss: 2017.5270\n",
            "Epoch [40/50], Loss: 2036.8235\n",
            "Epoch [41/50], Loss: 2024.7179\n",
            "Epoch [42/50], Loss: 2010.0667\n",
            "Epoch [43/50], Loss: 2013.3664\n",
            "Epoch [44/50], Loss: 2012.5931\n",
            "Epoch [45/50], Loss: 2012.7214\n",
            "Epoch [46/50], Loss: 2025.3690\n",
            "Epoch [47/50], Loss: 2007.3523\n",
            "Epoch [48/50], Loss: 2008.5832\n",
            "Epoch [49/50], Loss: 2028.1724\n",
            "Epoch [50/50], Loss: 2016.0985\n",
            "Mean Squared Error: 11755.5829\n",
            "Epoch [1/50], Loss: 2071.9316\n",
            "Epoch [2/50], Loss: 2014.0688\n",
            "Epoch [3/50], Loss: 2099.2842\n",
            "Epoch [4/50], Loss: 2046.0164\n",
            "Epoch [5/50], Loss: 2046.9719\n",
            "Epoch [6/50], Loss: 2004.5522\n",
            "Epoch [7/50], Loss: 1945.1343\n",
            "Epoch [8/50], Loss: 1978.8125\n",
            "Epoch [9/50], Loss: 1989.6850\n",
            "Epoch [10/50], Loss: 2103.4973\n",
            "Epoch [11/50], Loss: 1917.2535\n",
            "Epoch [12/50], Loss: 2112.9437\n",
            "Epoch [13/50], Loss: 1920.3501\n",
            "Epoch [14/50], Loss: 1906.1061\n",
            "Epoch [15/50], Loss: 3345.7116\n",
            "Epoch [16/50], Loss: 1930.4908\n",
            "Epoch [17/50], Loss: 2001.6662\n",
            "Epoch [18/50], Loss: 1901.9612\n",
            "Epoch [19/50], Loss: 2103.4463\n",
            "Epoch [20/50], Loss: 1906.8025\n",
            "Epoch [21/50], Loss: 1907.7831\n",
            "Epoch [22/50], Loss: 2019.3004\n",
            "Epoch [23/50], Loss: 3332.5334\n",
            "Epoch [24/50], Loss: 1895.9386\n",
            "Epoch [25/50], Loss: 1908.1253\n",
            "Epoch [26/50], Loss: 1911.8743\n",
            "Epoch [27/50], Loss: 1892.6849\n",
            "Epoch [28/50], Loss: 2059.2440\n",
            "Epoch [29/50], Loss: 1982.7330\n",
            "Epoch [30/50], Loss: 3376.1398\n",
            "Epoch [31/50], Loss: 1906.3384\n",
            "Epoch [32/50], Loss: 1899.7711\n",
            "Epoch [33/50], Loss: 1915.4098\n",
            "Epoch [34/50], Loss: 1925.3681\n",
            "Epoch [35/50], Loss: 1894.5420\n",
            "Epoch [36/50], Loss: 1961.1292\n",
            "Epoch [37/50], Loss: 1970.7100\n",
            "Epoch [38/50], Loss: 1914.9341\n",
            "Epoch [39/50], Loss: 1883.6027\n",
            "Epoch [40/50], Loss: 2088.0158\n",
            "Epoch [41/50], Loss: 1885.8752\n",
            "Epoch [42/50], Loss: 1886.8285\n",
            "Epoch [43/50], Loss: 1888.0545\n",
            "Epoch [44/50], Loss: 1978.8032\n",
            "Epoch [45/50], Loss: 2151.6816\n",
            "Epoch [46/50], Loss: 1887.6886\n",
            "Epoch [47/50], Loss: 1883.4712\n",
            "Epoch [48/50], Loss: 1892.2489\n",
            "Epoch [49/50], Loss: 1895.9016\n",
            "Epoch [50/50], Loss: 1904.4985\n",
            "Mean Squared Error: 11787.9338\n",
            "Epoch [1/50], Loss: 2121.9480\n",
            "Epoch [2/50], Loss: 2038.2782\n",
            "Epoch [3/50], Loss: 1788.8176\n",
            "Epoch [4/50], Loss: 1814.8371\n",
            "Epoch [5/50], Loss: 2292.7807\n",
            "Epoch [6/50], Loss: 5454.2977\n",
            "Epoch [7/50], Loss: 1773.3586\n",
            "Epoch [8/50], Loss: 1730.7267\n",
            "Epoch [9/50], Loss: 1742.4049\n",
            "Epoch [10/50], Loss: 1750.8682\n",
            "Epoch [11/50], Loss: 1920.9861\n",
            "Epoch [12/50], Loss: 1697.8574\n",
            "Epoch [13/50], Loss: 1954.3112\n",
            "Epoch [14/50], Loss: 1896.2417\n",
            "Epoch [15/50], Loss: 2407.9977\n",
            "Epoch [16/50], Loss: 1722.0410\n",
            "Epoch [17/50], Loss: 1687.4625\n",
            "Epoch [18/50], Loss: 1708.4859\n",
            "Epoch [19/50], Loss: 1756.3396\n",
            "Epoch [20/50], Loss: 1701.3545\n",
            "Epoch [21/50], Loss: 1943.7549\n",
            "Epoch [22/50], Loss: 1721.2364\n",
            "Epoch [23/50], Loss: 2171.5427\n",
            "Epoch [24/50], Loss: 1704.2907\n",
            "Epoch [25/50], Loss: 1700.5696\n",
            "Epoch [26/50], Loss: 1692.7806\n",
            "Epoch [27/50], Loss: 1731.2505\n",
            "Epoch [28/50], Loss: 1686.3502\n",
            "Epoch [29/50], Loss: 1681.0872\n",
            "Epoch [30/50], Loss: 1734.3821\n",
            "Epoch [31/50], Loss: 1695.8522\n",
            "Epoch [32/50], Loss: 1912.2953\n",
            "Epoch [33/50], Loss: 1690.2758\n",
            "Epoch [34/50], Loss: 1719.0377\n",
            "Epoch [35/50], Loss: 1679.0202\n",
            "Epoch [36/50], Loss: 1681.1255\n",
            "Epoch [37/50], Loss: 1681.5683\n",
            "Epoch [38/50], Loss: 1961.5453\n",
            "Epoch [39/50], Loss: 2160.0022\n",
            "Epoch [40/50], Loss: 1851.5375\n",
            "Epoch [41/50], Loss: 1826.8928\n",
            "Epoch [42/50], Loss: 2327.0677\n",
            "Epoch [43/50], Loss: 1676.0856\n",
            "Epoch [44/50], Loss: 1859.5738\n",
            "Epoch [45/50], Loss: 1827.7921\n",
            "Epoch [46/50], Loss: 1774.8619\n",
            "Epoch [47/50], Loss: 1911.7036\n",
            "Epoch [48/50], Loss: 1687.3238\n",
            "Epoch [49/50], Loss: 1935.7845\n",
            "Epoch [50/50], Loss: 1700.5445\n",
            "Mean Squared Error: 11803.4037\n",
            "Epoch [1/50], Loss: 1961.9689\n",
            "Epoch [2/50], Loss: 1894.2886\n",
            "Epoch [3/50], Loss: 2625.5292\n",
            "Epoch [4/50], Loss: 2036.8986\n",
            "Epoch [5/50], Loss: 1900.4305\n",
            "Epoch [6/50], Loss: 2487.8622\n",
            "Epoch [7/50], Loss: 2549.1273\n",
            "Epoch [8/50], Loss: 1876.5156\n",
            "Epoch [9/50], Loss: 1890.1638\n",
            "Epoch [10/50], Loss: 2457.0547\n",
            "Epoch [11/50], Loss: 2005.7369\n",
            "Epoch [12/50], Loss: 1786.1454\n",
            "Epoch [13/50], Loss: 1798.9046\n",
            "Epoch [14/50], Loss: 1897.9012\n",
            "Epoch [15/50], Loss: 1821.5659\n",
            "Epoch [16/50], Loss: 2542.9727\n",
            "Epoch [17/50], Loss: 1856.1662\n",
            "Epoch [18/50], Loss: 2424.2309\n",
            "Epoch [19/50], Loss: 1896.2286\n",
            "Epoch [20/50], Loss: 2429.2844\n",
            "Epoch [21/50], Loss: 1967.1455\n",
            "Epoch [22/50], Loss: 2491.5610\n",
            "Epoch [23/50], Loss: 1773.5633\n",
            "Epoch [24/50], Loss: 2504.9540\n",
            "Epoch [25/50], Loss: 1881.5188\n",
            "Epoch [26/50], Loss: 1740.0774\n",
            "Epoch [27/50], Loss: 2473.1711\n",
            "Epoch [28/50], Loss: 2441.4608\n",
            "Epoch [29/50], Loss: 1795.2303\n",
            "Epoch [30/50], Loss: 2404.4019\n",
            "Epoch [31/50], Loss: 2432.9709\n",
            "Epoch [32/50], Loss: 1704.1708\n",
            "Epoch [33/50], Loss: 2507.5135\n",
            "Epoch [34/50], Loss: 1815.6064\n",
            "Epoch [35/50], Loss: 2399.2964\n",
            "Epoch [36/50], Loss: 2372.5105\n",
            "Epoch [37/50], Loss: 2509.1753\n",
            "Epoch [38/50], Loss: 1822.7638\n",
            "Epoch [39/50], Loss: 2585.1102\n",
            "Epoch [40/50], Loss: 2539.8400\n",
            "Epoch [41/50], Loss: 1735.3910\n",
            "Epoch [42/50], Loss: 1830.6489\n",
            "Epoch [43/50], Loss: 2450.7576\n",
            "Epoch [44/50], Loss: 2383.8120\n",
            "Epoch [45/50], Loss: 1817.7019\n",
            "Epoch [46/50], Loss: 2451.3564\n",
            "Epoch [47/50], Loss: 1854.5120\n",
            "Epoch [48/50], Loss: 1692.7841\n",
            "Epoch [49/50], Loss: 1730.6073\n",
            "Epoch [50/50], Loss: 1773.2882\n",
            "Mean Squared Error: 11806.2568\n",
            "Epoch [1/50], Loss: 2204.2825\n",
            "Epoch [2/50], Loss: 2201.6794\n",
            "Epoch [3/50], Loss: 2199.0847\n",
            "Epoch [4/50], Loss: 2196.5845\n",
            "Epoch [5/50], Loss: 2193.9729\n",
            "Epoch [6/50], Loss: 2191.1536\n",
            "Epoch [7/50], Loss: 2188.1096\n",
            "Epoch [8/50], Loss: 2184.8447\n",
            "Epoch [9/50], Loss: 2181.3533\n",
            "Epoch [10/50], Loss: 2177.6069\n",
            "Epoch [11/50], Loss: 2173.6172\n",
            "Epoch [12/50], Loss: 2169.4062\n",
            "Epoch [13/50], Loss: 2164.9961\n",
            "Epoch [14/50], Loss: 2160.4011\n",
            "Epoch [15/50], Loss: 2155.6362\n",
            "Epoch [16/50], Loss: 2150.7185\n",
            "Epoch [17/50], Loss: 2145.6663\n",
            "Epoch [18/50], Loss: 2140.5024\n",
            "Epoch [19/50], Loss: 2135.2507\n",
            "Epoch [20/50], Loss: 2129.9392\n",
            "Epoch [21/50], Loss: 2124.5977\n",
            "Epoch [22/50], Loss: 2119.2585\n",
            "Epoch [23/50], Loss: 2113.9580\n",
            "Epoch [24/50], Loss: 2108.7327\n",
            "Epoch [25/50], Loss: 2103.6221\n",
            "Epoch [26/50], Loss: 2098.6670\n",
            "Epoch [27/50], Loss: 2093.9084\n",
            "Epoch [28/50], Loss: 2089.3875\n",
            "Epoch [29/50], Loss: 2085.1438\n",
            "Epoch [30/50], Loss: 2081.2151\n",
            "Epoch [31/50], Loss: 2077.6345\n",
            "Epoch [32/50], Loss: 2074.4307\n",
            "Epoch [33/50], Loss: 2071.6243\n",
            "Epoch [34/50], Loss: 2069.2292\n",
            "Epoch [35/50], Loss: 2067.2473\n",
            "Epoch [36/50], Loss: 2065.6694\n",
            "Epoch [37/50], Loss: 2064.4756\n",
            "Epoch [38/50], Loss: 2063.6294\n",
            "Epoch [39/50], Loss: 2063.0857\n",
            "Epoch [40/50], Loss: 2062.7893\n",
            "Epoch [41/50], Loss: 2062.6753\n",
            "Epoch [42/50], Loss: 2062.6785\n",
            "Epoch [43/50], Loss: 2062.7334\n",
            "Epoch [44/50], Loss: 2062.7827\n",
            "Epoch [45/50], Loss: 2062.7769\n",
            "Epoch [46/50], Loss: 2062.6809\n",
            "Epoch [47/50], Loss: 2062.4734\n",
            "Epoch [48/50], Loss: 2062.1455\n",
            "Epoch [49/50], Loss: 2061.7029\n",
            "Epoch [50/50], Loss: 2061.1589\n",
            "Mean Squared Error: 11807.8402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Epoch Comparison\n",
        "results_epochs = []\n",
        "for epochs in epochs_list:\n",
        "    train_loader = create_data_loader(X_train_tensor, y_train_tensor, batch_size=32)\n",
        "    test_loader = create_data_loader(X_test_tensor, y_test_tensor, batch_size=32)\n",
        "\n",
        "    model = MLPRegressionModel(input_size, [16], nn.ReLU())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    mse = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n",
        "\n",
        "    results_epochs.append({\n",
        "        'epochs': epochs,\n",
        "        'mse': mse\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvYgS34Rz_Xz",
        "outputId": "938f9697-c66b-48fe-c9ab-8929863493b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 2176.8658\n",
            "Mean Squared Error: 12086.6834\n",
            "Epoch [1/10], Loss: 2185.4537\n",
            "Epoch [2/10], Loss: 2156.3938\n",
            "Epoch [3/10], Loss: 2131.3225\n",
            "Epoch [4/10], Loss: 2076.0725\n",
            "Epoch [5/10], Loss: 2058.2880\n",
            "Epoch [6/10], Loss: 2064.0842\n",
            "Epoch [7/10], Loss: 2060.0124\n",
            "Epoch [8/10], Loss: 2073.1674\n",
            "Epoch [9/10], Loss: 2053.9589\n",
            "Epoch [10/10], Loss: 2050.0858\n",
            "Mean Squared Error: 11817.0617\n",
            "Epoch [1/25], Loss: 2178.7328\n",
            "Epoch [2/25], Loss: 2152.3079\n",
            "Epoch [3/25], Loss: 2091.7031\n",
            "Epoch [4/25], Loss: 2076.9306\n",
            "Epoch [5/25], Loss: 2061.9851\n",
            "Epoch [6/25], Loss: 2050.1971\n",
            "Epoch [7/25], Loss: 2046.6378\n",
            "Epoch [8/25], Loss: 2047.3226\n",
            "Epoch [9/25], Loss: 2050.7280\n",
            "Epoch [10/25], Loss: 2175.4586\n",
            "Epoch [11/25], Loss: 2043.1458\n",
            "Epoch [12/25], Loss: 2037.9360\n",
            "Epoch [13/25], Loss: 2039.8198\n",
            "Epoch [14/25], Loss: 2039.2078\n",
            "Epoch [15/25], Loss: 2043.8674\n",
            "Epoch [16/25], Loss: 2043.3031\n",
            "Epoch [17/25], Loss: 2033.9929\n",
            "Epoch [18/25], Loss: 2050.2745\n",
            "Epoch [19/25], Loss: 2047.8202\n",
            "Epoch [20/25], Loss: 2032.3147\n",
            "Epoch [21/25], Loss: 2040.2622\n",
            "Epoch [22/25], Loss: 2159.7277\n",
            "Epoch [23/25], Loss: 2025.5855\n",
            "Epoch [24/25], Loss: 2024.8586\n",
            "Epoch [25/25], Loss: 2024.5954\n",
            "Mean Squared Error: 11786.2539\n",
            "Epoch [1/50], Loss: 2203.9484\n",
            "Epoch [2/50], Loss: 2165.3400\n",
            "Epoch [3/50], Loss: 2135.8795\n",
            "Epoch [4/50], Loss: 2208.8442\n",
            "Epoch [5/50], Loss: 2066.4974\n",
            "Epoch [6/50], Loss: 2050.7145\n",
            "Epoch [7/50], Loss: 2068.7335\n",
            "Epoch [8/50], Loss: 2055.1016\n",
            "Epoch [9/50], Loss: 2052.5029\n",
            "Epoch [10/50], Loss: 2043.4905\n",
            "Epoch [11/50], Loss: 2041.4005\n",
            "Epoch [12/50], Loss: 2172.4906\n",
            "Epoch [13/50], Loss: 2041.1408\n",
            "Epoch [14/50], Loss: 2044.7296\n",
            "Epoch [15/50], Loss: 2187.0911\n",
            "Epoch [16/50], Loss: 2185.5545\n",
            "Epoch [17/50], Loss: 2033.6238\n",
            "Epoch [18/50], Loss: 2034.0666\n",
            "Epoch [19/50], Loss: 2051.9154\n",
            "Epoch [20/50], Loss: 2033.7616\n",
            "Epoch [21/50], Loss: 2030.4791\n",
            "Epoch [22/50], Loss: 2044.1365\n",
            "Epoch [23/50], Loss: 2029.9789\n",
            "Epoch [24/50], Loss: 2027.9558\n",
            "Epoch [25/50], Loss: 2028.0617\n",
            "Epoch [26/50], Loss: 2049.8286\n",
            "Epoch [27/50], Loss: 2173.5839\n",
            "Epoch [28/50], Loss: 2026.6267\n",
            "Epoch [29/50], Loss: 2030.1563\n",
            "Epoch [30/50], Loss: 2022.1311\n",
            "Epoch [31/50], Loss: 2021.2566\n",
            "Epoch [32/50], Loss: 2022.2348\n",
            "Epoch [33/50], Loss: 2036.5018\n",
            "Epoch [34/50], Loss: 2036.4993\n",
            "Epoch [35/50], Loss: 2149.6732\n",
            "Epoch [36/50], Loss: 2147.6955\n",
            "Epoch [37/50], Loss: 2018.3452\n",
            "Epoch [38/50], Loss: 2148.0468\n",
            "Epoch [39/50], Loss: 2019.1330\n",
            "Epoch [40/50], Loss: 2015.2758\n",
            "Epoch [41/50], Loss: 2017.3521\n",
            "Epoch [42/50], Loss: 2015.0876\n",
            "Epoch [43/50], Loss: 2143.1749\n",
            "Epoch [44/50], Loss: 2021.1702\n",
            "Epoch [45/50], Loss: 2142.5703\n",
            "Epoch [46/50], Loss: 2011.0116\n",
            "Epoch [47/50], Loss: 2011.9707\n",
            "Epoch [48/50], Loss: 2028.3795\n",
            "Epoch [49/50], Loss: 2020.8220\n",
            "Epoch [50/50], Loss: 2010.5106\n",
            "Mean Squared Error: 11781.1371\n",
            "Epoch [1/100], Loss: 2189.6933\n",
            "Epoch [2/100], Loss: 2153.6271\n",
            "Epoch [3/100], Loss: 2112.0621\n",
            "Epoch [4/100], Loss: 2076.9595\n",
            "Epoch [5/100], Loss: 2073.3715\n",
            "Epoch [6/100], Loss: 2065.2622\n",
            "Epoch [7/100], Loss: 2064.8668\n",
            "Epoch [8/100], Loss: 2049.0244\n",
            "Epoch [9/100], Loss: 2046.1606\n",
            "Epoch [10/100], Loss: 2049.6759\n",
            "Epoch [11/100], Loss: 2045.5269\n",
            "Epoch [12/100], Loss: 2055.6461\n",
            "Epoch [13/100], Loss: 2049.0964\n",
            "Epoch [14/100], Loss: 2038.3767\n",
            "Epoch [15/100], Loss: 2046.5113\n",
            "Epoch [16/100], Loss: 2056.0946\n",
            "Epoch [17/100], Loss: 2038.3297\n",
            "Epoch [18/100], Loss: 2033.5080\n",
            "Epoch [19/100], Loss: 2035.7487\n",
            "Epoch [20/100], Loss: 2042.8716\n",
            "Epoch [21/100], Loss: 2036.2613\n",
            "Epoch [22/100], Loss: 2050.7483\n",
            "Epoch [23/100], Loss: 2042.4601\n",
            "Epoch [24/100], Loss: 2028.4153\n",
            "Epoch [25/100], Loss: 2039.0958\n",
            "Epoch [26/100], Loss: 2040.3854\n",
            "Epoch [27/100], Loss: 2026.7662\n",
            "Epoch [28/100], Loss: 2027.6661\n",
            "Epoch [29/100], Loss: 2026.2903\n",
            "Epoch [30/100], Loss: 2156.4843\n",
            "Epoch [31/100], Loss: 2026.8736\n",
            "Epoch [32/100], Loss: 2035.2402\n",
            "Epoch [33/100], Loss: 2031.4715\n",
            "Epoch [34/100], Loss: 2043.9279\n",
            "Epoch [35/100], Loss: 2032.3602\n",
            "Epoch [36/100], Loss: 2019.2013\n",
            "Epoch [37/100], Loss: 2034.3846\n",
            "Epoch [38/100], Loss: 2021.1018\n",
            "Epoch [39/100], Loss: 2163.8852\n",
            "Epoch [40/100], Loss: 2018.3912\n",
            "Epoch [41/100], Loss: 2028.3073\n",
            "Epoch [42/100], Loss: 2030.1236\n",
            "Epoch [43/100], Loss: 2017.6018\n",
            "Epoch [44/100], Loss: 2014.2691\n",
            "Epoch [45/100], Loss: 2021.6538\n",
            "Epoch [46/100], Loss: 2014.3935\n",
            "Epoch [47/100], Loss: 2040.1606\n",
            "Epoch [48/100], Loss: 2021.3281\n",
            "Epoch [49/100], Loss: 2025.4912\n",
            "Epoch [50/100], Loss: 2018.9609\n",
            "Epoch [51/100], Loss: 2014.7917\n",
            "Epoch [52/100], Loss: 2010.1904\n",
            "Epoch [53/100], Loss: 2009.2126\n",
            "Epoch [54/100], Loss: 2009.3859\n",
            "Epoch [55/100], Loss: 2011.6800\n",
            "Epoch [56/100], Loss: 2021.4517\n",
            "Epoch [57/100], Loss: 2015.5798\n",
            "Epoch [58/100], Loss: 2016.5503\n",
            "Epoch [59/100], Loss: 2007.0336\n",
            "Epoch [60/100], Loss: 2006.0398\n",
            "Epoch [61/100], Loss: 2004.2689\n",
            "Epoch [62/100], Loss: 2015.0514\n",
            "Epoch [63/100], Loss: 2004.6336\n",
            "Epoch [64/100], Loss: 2011.5198\n",
            "Epoch [65/100], Loss: 2009.5337\n",
            "Epoch [66/100], Loss: 2008.5276\n",
            "Epoch [67/100], Loss: 2018.4529\n",
            "Epoch [68/100], Loss: 2003.2730\n",
            "Epoch [69/100], Loss: 2009.4270\n",
            "Epoch [70/100], Loss: 2002.0918\n",
            "Epoch [71/100], Loss: 2002.0352\n",
            "Epoch [72/100], Loss: 1999.7487\n",
            "Epoch [73/100], Loss: 2150.1581\n",
            "Epoch [74/100], Loss: 2003.9797\n",
            "Epoch [75/100], Loss: 2000.7277\n",
            "Epoch [76/100], Loss: 2000.2528\n",
            "Epoch [77/100], Loss: 1999.6421\n",
            "Epoch [78/100], Loss: 1997.9814\n",
            "Epoch [79/100], Loss: 2153.0000\n",
            "Epoch [80/100], Loss: 1994.4292\n",
            "Epoch [81/100], Loss: 1996.6294\n",
            "Epoch [82/100], Loss: 1995.3022\n",
            "Epoch [83/100], Loss: 2012.8203\n",
            "Epoch [84/100], Loss: 1998.8467\n",
            "Epoch [85/100], Loss: 2132.2449\n",
            "Epoch [86/100], Loss: 1993.2321\n",
            "Epoch [87/100], Loss: 1994.6943\n",
            "Epoch [88/100], Loss: 2120.6888\n",
            "Epoch [89/100], Loss: 1995.9528\n",
            "Epoch [90/100], Loss: 1998.0660\n",
            "Epoch [91/100], Loss: 1991.6048\n",
            "Epoch [92/100], Loss: 1992.9198\n",
            "Epoch [93/100], Loss: 1990.6831\n",
            "Epoch [94/100], Loss: 1991.4373\n",
            "Epoch [95/100], Loss: 1991.5479\n",
            "Epoch [96/100], Loss: 2007.4431\n",
            "Epoch [97/100], Loss: 1998.6136\n",
            "Epoch [98/100], Loss: 1999.6797\n",
            "Epoch [99/100], Loss: 1989.1251\n",
            "Epoch [100/100], Loss: 2118.1848\n",
            "Mean Squared Error: 11748.0776\n",
            "Epoch [1/250], Loss: 2166.0829\n",
            "Epoch [2/250], Loss: 2114.9085\n",
            "Epoch [3/250], Loss: 2086.5758\n",
            "Epoch [4/250], Loss: 2059.9044\n",
            "Epoch [5/250], Loss: 2058.6108\n",
            "Epoch [6/250], Loss: 2048.9156\n",
            "Epoch [7/250], Loss: 2044.6844\n",
            "Epoch [8/250], Loss: 2052.7854\n",
            "Epoch [9/250], Loss: 2050.4710\n",
            "Epoch [10/250], Loss: 2058.0540\n",
            "Epoch [11/250], Loss: 2045.3631\n",
            "Epoch [12/250], Loss: 2038.4025\n",
            "Epoch [13/250], Loss: 2035.1075\n",
            "Epoch [14/250], Loss: 2049.4449\n",
            "Epoch [15/250], Loss: 2041.1208\n",
            "Epoch [16/250], Loss: 2034.0649\n",
            "Epoch [17/250], Loss: 2050.3986\n",
            "Epoch [18/250], Loss: 2162.8397\n",
            "Epoch [19/250], Loss: 2029.8326\n",
            "Epoch [20/250], Loss: 2036.2650\n",
            "Epoch [21/250], Loss: 2156.8514\n",
            "Epoch [22/250], Loss: 2028.4540\n",
            "Epoch [23/250], Loss: 2023.9470\n",
            "Epoch [24/250], Loss: 2156.6980\n",
            "Epoch [25/250], Loss: 2021.1525\n",
            "Epoch [26/250], Loss: 2018.9802\n",
            "Epoch [27/250], Loss: 2019.4609\n",
            "Epoch [28/250], Loss: 2025.7260\n",
            "Epoch [29/250], Loss: 2017.5504\n",
            "Epoch [30/250], Loss: 2023.1769\n",
            "Epoch [31/250], Loss: 2016.7214\n",
            "Epoch [32/250], Loss: 2028.8080\n",
            "Epoch [33/250], Loss: 2030.5478\n",
            "Epoch [34/250], Loss: 2014.3554\n",
            "Epoch [35/250], Loss: 2020.7121\n",
            "Epoch [36/250], Loss: 2027.0735\n",
            "Epoch [37/250], Loss: 2011.2778\n",
            "Epoch [38/250], Loss: 2023.3304\n",
            "Epoch [39/250], Loss: 2013.9106\n",
            "Epoch [40/250], Loss: 2011.5165\n",
            "Epoch [41/250], Loss: 2019.4553\n",
            "Epoch [42/250], Loss: 2013.0658\n",
            "Epoch [43/250], Loss: 2023.5562\n",
            "Epoch [44/250], Loss: 2006.1307\n",
            "Epoch [45/250], Loss: 2004.8413\n",
            "Epoch [46/250], Loss: 2138.3415\n",
            "Epoch [47/250], Loss: 2017.9942\n",
            "Epoch [48/250], Loss: 2008.0023\n",
            "Epoch [49/250], Loss: 1997.8710\n",
            "Epoch [50/250], Loss: 2001.0379\n",
            "Epoch [51/250], Loss: 1996.6729\n",
            "Epoch [52/250], Loss: 2016.9943\n",
            "Epoch [53/250], Loss: 2011.4080\n",
            "Epoch [54/250], Loss: 1998.5782\n",
            "Epoch [55/250], Loss: 2009.4769\n",
            "Epoch [56/250], Loss: 2162.2395\n",
            "Epoch [57/250], Loss: 1993.3484\n",
            "Epoch [58/250], Loss: 1995.9233\n",
            "Epoch [59/250], Loss: 1985.0224\n",
            "Epoch [60/250], Loss: 1986.4325\n",
            "Epoch [61/250], Loss: 1989.1041\n",
            "Epoch [62/250], Loss: 1987.2078\n",
            "Epoch [63/250], Loss: 2123.3250\n",
            "Epoch [64/250], Loss: 1980.2312\n",
            "Epoch [65/250], Loss: 1980.9221\n",
            "Epoch [66/250], Loss: 1980.2819\n",
            "Epoch [67/250], Loss: 1983.4380\n",
            "Epoch [68/250], Loss: 1976.2763\n",
            "Epoch [69/250], Loss: 1982.9067\n",
            "Epoch [70/250], Loss: 1987.0859\n",
            "Epoch [71/250], Loss: 1994.5501\n",
            "Epoch [72/250], Loss: 1978.1178\n",
            "Epoch [73/250], Loss: 1973.0943\n",
            "Epoch [74/250], Loss: 1975.4858\n",
            "Epoch [75/250], Loss: 1969.5665\n",
            "Epoch [76/250], Loss: 1972.6552\n",
            "Epoch [77/250], Loss: 1980.5579\n",
            "Epoch [78/250], Loss: 1978.5511\n",
            "Epoch [79/250], Loss: 1969.5784\n",
            "Epoch [80/250], Loss: 1987.8103\n",
            "Epoch [81/250], Loss: 1966.7858\n",
            "Epoch [82/250], Loss: 1975.5179\n",
            "Epoch [83/250], Loss: 1967.3636\n",
            "Epoch [84/250], Loss: 1969.6942\n",
            "Epoch [85/250], Loss: 1967.9512\n",
            "Epoch [86/250], Loss: 2105.4673\n",
            "Epoch [87/250], Loss: 2084.0602\n",
            "Epoch [88/250], Loss: 1959.5332\n",
            "Epoch [89/250], Loss: 1959.3089\n",
            "Epoch [90/250], Loss: 1964.6203\n",
            "Epoch [91/250], Loss: 1967.5613\n",
            "Epoch [92/250], Loss: 2089.8122\n",
            "Epoch [93/250], Loss: 2071.0283\n",
            "Epoch [94/250], Loss: 1972.0068\n",
            "Epoch [95/250], Loss: 1956.1356\n",
            "Epoch [96/250], Loss: 1956.6793\n",
            "Epoch [97/250], Loss: 1948.4036\n",
            "Epoch [98/250], Loss: 1946.3033\n",
            "Epoch [99/250], Loss: 1946.7130\n",
            "Epoch [100/250], Loss: 1953.4802\n",
            "Epoch [101/250], Loss: 1957.7011\n",
            "Epoch [102/250], Loss: 1955.6975\n",
            "Epoch [103/250], Loss: 1965.4510\n",
            "Epoch [104/250], Loss: 1946.0975\n",
            "Epoch [105/250], Loss: 1937.9867\n",
            "Epoch [106/250], Loss: 1948.3590\n",
            "Epoch [107/250], Loss: 1934.8206\n",
            "Epoch [108/250], Loss: 1936.5193\n",
            "Epoch [109/250], Loss: 1936.7792\n",
            "Epoch [110/250], Loss: 1931.8703\n",
            "Epoch [111/250], Loss: 1954.1161\n",
            "Epoch [112/250], Loss: 1939.0563\n",
            "Epoch [113/250], Loss: 1937.5406\n",
            "Epoch [114/250], Loss: 1933.0821\n",
            "Epoch [115/250], Loss: 1943.4045\n",
            "Epoch [116/250], Loss: 1936.1007\n",
            "Epoch [117/250], Loss: 1928.5558\n",
            "Epoch [118/250], Loss: 1928.8318\n",
            "Epoch [119/250], Loss: 1922.5924\n",
            "Epoch [120/250], Loss: 1920.6647\n",
            "Epoch [121/250], Loss: 1920.4600\n",
            "Epoch [122/250], Loss: 1919.9299\n",
            "Epoch [123/250], Loss: 1917.5359\n",
            "Epoch [124/250], Loss: 1921.0274\n",
            "Epoch [125/250], Loss: 1912.8482\n",
            "Epoch [126/250], Loss: 1913.0740\n",
            "Epoch [127/250], Loss: 1930.6844\n",
            "Epoch [128/250], Loss: 1921.6139\n",
            "Epoch [129/250], Loss: 1925.0221\n",
            "Epoch [130/250], Loss: 1925.1847\n",
            "Epoch [131/250], Loss: 1914.8183\n",
            "Epoch [132/250], Loss: 1913.9642\n",
            "Epoch [133/250], Loss: 1901.3060\n",
            "Epoch [134/250], Loss: 1912.8480\n",
            "Epoch [135/250], Loss: 1907.4144\n",
            "Epoch [136/250], Loss: 1904.4353\n",
            "Epoch [137/250], Loss: 1894.0354\n",
            "Epoch [138/250], Loss: 1902.3166\n",
            "Epoch [139/250], Loss: 1895.7097\n",
            "Epoch [140/250], Loss: 1897.0060\n",
            "Epoch [141/250], Loss: 1894.2240\n",
            "Epoch [142/250], Loss: 2015.5556\n",
            "Epoch [143/250], Loss: 1920.6245\n",
            "Epoch [144/250], Loss: 1889.8882\n",
            "Epoch [145/250], Loss: 1906.3489\n",
            "Epoch [146/250], Loss: 1887.7369\n",
            "Epoch [147/250], Loss: 1887.5256\n",
            "Epoch [148/250], Loss: 1889.0167\n",
            "Epoch [149/250], Loss: 1893.6827\n",
            "Epoch [150/250], Loss: 1902.9232\n",
            "Epoch [151/250], Loss: 1880.3336\n",
            "Epoch [152/250], Loss: 1878.4684\n",
            "Epoch [153/250], Loss: 1993.7636\n",
            "Epoch [154/250], Loss: 1879.4852\n",
            "Epoch [155/250], Loss: 1872.4033\n",
            "Epoch [156/250], Loss: 1869.8465\n",
            "Epoch [157/250], Loss: 1877.2375\n",
            "Epoch [158/250], Loss: 1890.0498\n",
            "Epoch [159/250], Loss: 1872.3571\n",
            "Epoch [160/250], Loss: 1875.1746\n",
            "Epoch [161/250], Loss: 1867.4478\n",
            "Epoch [162/250], Loss: 1877.3419\n",
            "Epoch [163/250], Loss: 1866.8927\n",
            "Epoch [164/250], Loss: 1989.1136\n",
            "Epoch [165/250], Loss: 1879.0803\n",
            "Epoch [166/250], Loss: 1865.0114\n",
            "Epoch [167/250], Loss: 1865.1086\n",
            "Epoch [168/250], Loss: 1868.4378\n",
            "Epoch [169/250], Loss: 1970.4240\n",
            "Epoch [170/250], Loss: 1984.8901\n",
            "Epoch [171/250], Loss: 1857.7261\n",
            "Epoch [172/250], Loss: 1869.5065\n",
            "Epoch [173/250], Loss: 1850.8776\n",
            "Epoch [174/250], Loss: 1857.2407\n",
            "Epoch [175/250], Loss: 1869.0934\n",
            "Epoch [176/250], Loss: 1852.2116\n",
            "Epoch [177/250], Loss: 1852.2755\n",
            "Epoch [178/250], Loss: 1866.5314\n",
            "Epoch [179/250], Loss: 1854.8964\n",
            "Epoch [180/250], Loss: 1853.9352\n",
            "Epoch [181/250], Loss: 1868.0364\n",
            "Epoch [182/250], Loss: 1850.9510\n",
            "Epoch [183/250], Loss: 1842.7850\n",
            "Epoch [184/250], Loss: 1848.7195\n",
            "Epoch [185/250], Loss: 1858.7764\n",
            "Epoch [186/250], Loss: 1840.0754\n",
            "Epoch [187/250], Loss: 1833.3652\n",
            "Epoch [188/250], Loss: 1963.9642\n",
            "Epoch [189/250], Loss: 1839.0444\n",
            "Epoch [190/250], Loss: 1849.7326\n",
            "Epoch [191/250], Loss: 1834.4175\n",
            "Epoch [192/250], Loss: 1830.9212\n",
            "Epoch [193/250], Loss: 1839.3278\n",
            "Epoch [194/250], Loss: 1830.7085\n",
            "Epoch [195/250], Loss: 1841.9923\n",
            "Epoch [196/250], Loss: 1823.1337\n",
            "Epoch [197/250], Loss: 1840.9826\n",
            "Epoch [198/250], Loss: 1836.1699\n",
            "Epoch [199/250], Loss: 1829.7242\n",
            "Epoch [200/250], Loss: 1831.0655\n",
            "Epoch [201/250], Loss: 1822.0756\n",
            "Epoch [202/250], Loss: 1827.9386\n",
            "Epoch [203/250], Loss: 1822.6773\n",
            "Epoch [204/250], Loss: 1957.0818\n",
            "Epoch [205/250], Loss: 1934.6001\n",
            "Epoch [206/250], Loss: 1814.6340\n",
            "Epoch [207/250], Loss: 1816.6680\n",
            "Epoch [208/250], Loss: 1814.1430\n",
            "Epoch [209/250], Loss: 1812.3813\n",
            "Epoch [210/250], Loss: 1823.1863\n",
            "Epoch [211/250], Loss: 1811.3193\n",
            "Epoch [212/250], Loss: 1811.2604\n",
            "Epoch [213/250], Loss: 1817.4282\n",
            "Epoch [214/250], Loss: 1820.3919\n",
            "Epoch [215/250], Loss: 1829.6393\n",
            "Epoch [216/250], Loss: 1829.6001\n",
            "Epoch [217/250], Loss: 1813.2316\n",
            "Epoch [218/250], Loss: 1804.4937\n",
            "Epoch [219/250], Loss: 1810.9013\n",
            "Epoch [220/250], Loss: 1809.1572\n",
            "Epoch [221/250], Loss: 1812.7966\n",
            "Epoch [222/250], Loss: 1807.2585\n",
            "Epoch [223/250], Loss: 1805.8357\n",
            "Epoch [224/250], Loss: 1800.8568\n",
            "Epoch [225/250], Loss: 1805.1396\n",
            "Epoch [226/250], Loss: 1792.6943\n",
            "Epoch [227/250], Loss: 1795.0221\n",
            "Epoch [228/250], Loss: 1792.2524\n",
            "Epoch [229/250], Loss: 1804.1101\n",
            "Epoch [230/250], Loss: 1804.4718\n",
            "Epoch [231/250], Loss: 1807.0133\n",
            "Epoch [232/250], Loss: 1801.9327\n",
            "Epoch [233/250], Loss: 1793.0537\n",
            "Epoch [234/250], Loss: 1794.6480\n",
            "Epoch [235/250], Loss: 1800.2657\n",
            "Epoch [236/250], Loss: 1787.1179\n",
            "Epoch [237/250], Loss: 1796.5094\n",
            "Epoch [238/250], Loss: 1791.5650\n",
            "Epoch [239/250], Loss: 1788.5976\n",
            "Epoch [240/250], Loss: 1792.1079\n",
            "Epoch [241/250], Loss: 1806.2392\n",
            "Epoch [242/250], Loss: 1782.3396\n",
            "Epoch [243/250], Loss: 1786.6405\n",
            "Epoch [244/250], Loss: 1786.0794\n",
            "Epoch [245/250], Loss: 1784.8900\n",
            "Epoch [246/250], Loss: 1779.5331\n",
            "Epoch [247/250], Loss: 1777.2841\n",
            "Epoch [248/250], Loss: 1786.3642\n",
            "Epoch [249/250], Loss: 1787.1317\n",
            "Epoch [250/250], Loss: 1776.8624\n",
            "Mean Squared Error: 11746.5845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results for each comparison\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Results: Hidden Layer Comparison\")\n",
        "print(pd.DataFrame(results_hidden_layers))\n",
        "\n",
        "print(\"\\nResults: Activation Function Comparison\")\n",
        "print(pd.DataFrame(results_activation))\n",
        "\n",
        "print(\"\\nResults: Learning Rate Comparison\")\n",
        "print(pd.DataFrame(results_learning_rate))\n",
        "\n",
        "print(\"\\nResults: Batch Size Comparison\")\n",
        "print(pd.DataFrame(results_batch_size))\n",
        "\n",
        "print(\"\\nResults: Epoch Comparison\")\n",
        "print(pd.DataFrame(results_epochs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_I5ekaK0B1n",
        "outputId": "d0e54090-af8a-41c6-d9dc-18a5cd21676d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: Hidden Layer Comparison\n",
            "  hidden_layers           mse\n",
            "0           [4]  11798.659960\n",
            "1           [8]  11767.073436\n",
            "2          [16]  11775.520470\n",
            "3          [32]  11763.536288\n",
            "4          [64]  11709.271209\n",
            "\n",
            "Results: Activation Function Comparison\n",
            "  activation           mse\n",
            "0     Linear  11729.491443\n",
            "1    Sigmoid  11731.352447\n",
            "2       ReLU  11732.400958\n",
            "3    Softmax  11916.532374\n",
            "4       Tanh  11672.837015\n",
            "\n",
            "Results: Learning Rate Comparison\n",
            "   learning_rate           mse\n",
            "0        10.0000  11864.664954\n",
            "1         1.0000  11853.044377\n",
            "2         0.1000  11621.992726\n",
            "3         0.0100  11783.865333\n",
            "4         0.0010  11829.224948\n",
            "5         0.0001  12144.780508\n",
            "\n",
            "Results: Batch Size Comparison\n",
            "   batch_size           mse\n",
            "0          16  11729.955879\n",
            "1          32  11755.582891\n",
            "2          64  11787.933751\n",
            "3         128  11803.403688\n",
            "4         256  11806.256763\n",
            "5         512  11807.840175\n",
            "\n",
            "Results: Epoch Comparison\n",
            "   epochs           mse\n",
            "0       1  12086.683398\n",
            "1      10  11817.061654\n",
            "2      25  11786.253870\n",
            "3      50  11781.137060\n",
            "4     100  11748.077631\n",
            "5     250  11746.584513\n"
          ]
        }
      ]
    }
  ]
}