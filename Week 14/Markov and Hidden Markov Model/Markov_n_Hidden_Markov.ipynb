{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yaXvEtZvwabW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and clean the dataset\n",
        "file_path = '/content/AirQualityUCI.csv'\n",
        "data = pd.read_csv(file_path, delimiter=';')"
      ],
      "metadata": {
        "id": "s4isGxWkweYd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace commas with dots and convert to numeric\n",
        "for col in data.columns:\n",
        "    data[col] = data[col].replace({',': '.'}, regex=True).replace({'-200': np.nan})\n",
        "    try:\n",
        "        data[col] = pd.to_numeric(data[col])\n",
        "    except ValueError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "fGmnDhYxwge1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "data = data.drop(columns=[col for col in data.columns if 'Unnamed' in col])\n",
        "data = data.dropna()\n"
      ],
      "metadata": {
        "id": "k_Zsly0zwijd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features and target\n",
        "features = data.iloc[:, 2:].values\n",
        "target = data.iloc[:, 2].values"
      ],
      "metadata": {
        "id": "Fwn6WPlUwltF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "C8ro3HTEwn7T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Hyw3zZt2wp0u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset class\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, targets, sequence_length):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.sequence_length]\n",
        "        y = self.targets[idx + self.sequence_length]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "MuOoEp0IwqdS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Model Class\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Use the last hidden state\n",
        "        return out"
      ],
      "metadata": {
        "id": "jzIRH5JUwszM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation logic\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs, scheduler=None):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= len(dataloader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step(epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return train_losses"
      ],
      "metadata": {
        "id": "3CfRgrIcwwz1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "    return np.array(predictions), np.array(actuals)"
      ],
      "metadata": {
        "id": "nDyV4_hOwy5Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "sequence_length = 10\n",
        "hidden_sizes = [32, 64, 128]\n",
        "pooling_methods = ['max', 'avg']\n",
        "num_epochs_list = [5, 50, 100, 250, 350]\n",
        "optimizers = {'SGD': torch.optim.SGD, 'RMSProp': torch.optim.RMSprop, 'Adam': torch.optim.Adam}\n"
      ],
      "metadata": {
        "id": "9ZHPtvZPw1Bp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare datasets\n",
        "train_dataset = TimeSeriesDataset(X_train, y_train, sequence_length)\n",
        "test_dataset = TimeSeriesDataset(X_test, y_test, sequence_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "CDmCvn5Ow1wv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "mVdbDHtHw3r1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment by Hidden Size\n",
        "hidden_size_results = []\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "\n",
        "for hidden_size in hidden_sizes:\n",
        "    print(f\"\\nTraining with Hidden Size: {hidden_size}\")\n",
        "    model = RNNModel(input_size, hidden_size, output_size).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_losses = train_model(model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "    predictions, actuals = evaluate_model(model, test_loader)\n",
        "\n",
        "    hidden_size_results.append({\n",
        "        'hidden_size': hidden_size,\n",
        "        'train_losses': train_losses,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PebzYECR89vb",
        "outputId": "a6712c2a-38d9-4511-e216-7b889657f6c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Hidden Size: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 544.0543\n",
            "Epoch 2/50, Loss: 543.9733\n",
            "Epoch 3/50, Loss: 543.8724\n",
            "Epoch 4/50, Loss: 543.8733\n",
            "Epoch 5/50, Loss: 549.7223\n",
            "Epoch 6/50, Loss: 549.7887\n",
            "Epoch 7/50, Loss: 543.8348\n",
            "Epoch 8/50, Loss: 549.7265\n",
            "Epoch 9/50, Loss: 543.8219\n",
            "Epoch 10/50, Loss: 543.8522\n",
            "Epoch 11/50, Loss: 543.8349\n",
            "Epoch 12/50, Loss: 543.7846\n",
            "Epoch 13/50, Loss: 555.6043\n",
            "Epoch 14/50, Loss: 543.8800\n",
            "Epoch 15/50, Loss: 543.7971\n",
            "Epoch 16/50, Loss: 543.8803\n",
            "Epoch 17/50, Loss: 549.7101\n",
            "Epoch 18/50, Loss: 549.6907\n",
            "Epoch 19/50, Loss: 543.8462\n",
            "Epoch 20/50, Loss: 543.8107\n",
            "Epoch 21/50, Loss: 549.7107\n",
            "Epoch 22/50, Loss: 543.7966\n",
            "Epoch 23/50, Loss: 543.8137\n",
            "Epoch 24/50, Loss: 543.8315\n",
            "Epoch 25/50, Loss: 543.7949\n",
            "Epoch 26/50, Loss: 543.7853\n",
            "Epoch 27/50, Loss: 543.8059\n",
            "Epoch 28/50, Loss: 543.8106\n",
            "Epoch 29/50, Loss: 543.7909\n",
            "Epoch 30/50, Loss: 543.7954\n",
            "Epoch 31/50, Loss: 543.7781\n",
            "Epoch 32/50, Loss: 549.7109\n",
            "Epoch 33/50, Loss: 543.7928\n",
            "Epoch 34/50, Loss: 543.8376\n",
            "Epoch 35/50, Loss: 543.8074\n",
            "Epoch 36/50, Loss: 543.8014\n",
            "Epoch 37/50, Loss: 543.7757\n",
            "Epoch 38/50, Loss: 549.6734\n",
            "Epoch 39/50, Loss: 549.6989\n",
            "Epoch 40/50, Loss: 543.7641\n",
            "Epoch 41/50, Loss: 543.7673\n",
            "Epoch 42/50, Loss: 543.7859\n",
            "Epoch 43/50, Loss: 543.7914\n",
            "Epoch 44/50, Loss: 543.7879\n",
            "Epoch 45/50, Loss: 543.7850\n",
            "Epoch 46/50, Loss: 543.7824\n",
            "Epoch 47/50, Loss: 543.7704\n",
            "Epoch 48/50, Loss: 543.7985\n",
            "Epoch 49/50, Loss: 549.6786\n",
            "Epoch 50/50, Loss: 543.7627\n",
            "\n",
            "Training with Hidden Size: 64\n",
            "Epoch 1/50, Loss: 543.9910\n",
            "Epoch 2/50, Loss: 543.9053\n",
            "Epoch 3/50, Loss: 543.8743\n",
            "Epoch 4/50, Loss: 549.7632\n",
            "Epoch 5/50, Loss: 549.7629\n",
            "Epoch 6/50, Loss: 549.6964\n",
            "Epoch 7/50, Loss: 543.8705\n",
            "Epoch 8/50, Loss: 543.7945\n",
            "Epoch 9/50, Loss: 543.8257\n",
            "Epoch 10/50, Loss: 549.7154\n",
            "Epoch 11/50, Loss: 543.8230\n",
            "Epoch 12/50, Loss: 543.8194\n",
            "Epoch 13/50, Loss: 549.7052\n",
            "Epoch 14/50, Loss: 543.8285\n",
            "Epoch 15/50, Loss: 543.7959\n",
            "Epoch 16/50, Loss: 543.7758\n",
            "Epoch 17/50, Loss: 543.8374\n",
            "Epoch 18/50, Loss: 543.7952\n",
            "Epoch 19/50, Loss: 543.7878\n",
            "Epoch 20/50, Loss: 543.7838\n",
            "Epoch 21/50, Loss: 549.6979\n",
            "Epoch 22/50, Loss: 543.7739\n",
            "Epoch 23/50, Loss: 543.7972\n",
            "Epoch 24/50, Loss: 543.8013\n",
            "Epoch 25/50, Loss: 549.7111\n",
            "Epoch 26/50, Loss: 543.7885\n",
            "Epoch 27/50, Loss: 543.7745\n",
            "Epoch 28/50, Loss: 543.7707\n",
            "Epoch 29/50, Loss: 543.8152\n",
            "Epoch 30/50, Loss: 543.7825\n",
            "Epoch 31/50, Loss: 543.8079\n",
            "Epoch 32/50, Loss: 543.8047\n",
            "Epoch 33/50, Loss: 549.7112\n",
            "Epoch 34/50, Loss: 543.7791\n",
            "Epoch 35/50, Loss: 543.8291\n",
            "Epoch 36/50, Loss: 549.7520\n",
            "Epoch 37/50, Loss: 544.0173\n",
            "Epoch 38/50, Loss: 543.9194\n",
            "Epoch 39/50, Loss: 549.8389\n",
            "Epoch 40/50, Loss: 543.8855\n",
            "Epoch 41/50, Loss: 544.0054\n",
            "Epoch 42/50, Loss: 543.8773\n",
            "Epoch 43/50, Loss: 549.7587\n",
            "Epoch 44/50, Loss: 543.8682\n",
            "Epoch 45/50, Loss: 543.8519\n",
            "Epoch 46/50, Loss: 543.9368\n",
            "Epoch 47/50, Loss: 543.9057\n",
            "Epoch 48/50, Loss: 543.8789\n",
            "Epoch 49/50, Loss: 543.8953\n",
            "Epoch 50/50, Loss: 549.7300\n",
            "\n",
            "Training with Hidden Size: 128\n",
            "Epoch 1/50, Loss: 550.0523\n",
            "Epoch 2/50, Loss: 543.9837\n",
            "Epoch 3/50, Loss: 549.7725\n",
            "Epoch 4/50, Loss: 543.9300\n",
            "Epoch 5/50, Loss: 543.9745\n",
            "Epoch 6/50, Loss: 549.7990\n",
            "Epoch 7/50, Loss: 543.9000\n",
            "Epoch 8/50, Loss: 544.0986\n",
            "Epoch 9/50, Loss: 544.0695\n",
            "Epoch 10/50, Loss: 543.9419\n",
            "Epoch 11/50, Loss: 543.9550\n",
            "Epoch 12/50, Loss: 543.9690\n",
            "Epoch 13/50, Loss: 543.9248\n",
            "Epoch 14/50, Loss: 543.9742\n",
            "Epoch 15/50, Loss: 543.9028\n",
            "Epoch 16/50, Loss: 543.9181\n",
            "Epoch 17/50, Loss: 543.8933\n",
            "Epoch 18/50, Loss: 543.8699\n",
            "Epoch 19/50, Loss: 543.8759\n",
            "Epoch 20/50, Loss: 543.8466\n",
            "Epoch 21/50, Loss: 543.8430\n",
            "Epoch 22/50, Loss: 543.8673\n",
            "Epoch 23/50, Loss: 543.8010\n",
            "Epoch 24/50, Loss: 543.8849\n",
            "Epoch 25/50, Loss: 549.8395\n",
            "Epoch 26/50, Loss: 543.8598\n",
            "Epoch 27/50, Loss: 543.9031\n",
            "Epoch 28/50, Loss: 543.9380\n",
            "Epoch 29/50, Loss: 549.7236\n",
            "Epoch 30/50, Loss: 543.8337\n",
            "Epoch 31/50, Loss: 543.8445\n",
            "Epoch 32/50, Loss: 543.8392\n",
            "Epoch 33/50, Loss: 543.7951\n",
            "Epoch 34/50, Loss: 549.7392\n",
            "Epoch 35/50, Loss: 543.8636\n",
            "Epoch 36/50, Loss: 543.9099\n",
            "Epoch 37/50, Loss: 543.8447\n",
            "Epoch 38/50, Loss: 543.9019\n",
            "Epoch 39/50, Loss: 543.7902\n",
            "Epoch 40/50, Loss: 543.8751\n",
            "Epoch 41/50, Loss: 543.8008\n",
            "Epoch 42/50, Loss: 543.8179\n",
            "Epoch 43/50, Loss: 549.6648\n",
            "Epoch 44/50, Loss: 543.8441\n",
            "Epoch 45/50, Loss: 543.8275\n",
            "Epoch 46/50, Loss: 543.8687\n",
            "Epoch 47/50, Loss: 543.8123\n",
            "Epoch 48/50, Loss: 543.8393\n",
            "Epoch 49/50, Loss: 543.8271\n",
            "Epoch 50/50, Loss: 543.8814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment by Pooling Method\n",
        "pooling_results = []\n",
        "for pooling in pooling_methods:\n",
        "    print(f\"\\nTraining with Pooling Method: {pooling}\")\n",
        "    model = RNNModel(input_size, 64, output_size).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_losses = train_model(model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "    predictions, actuals = evaluate_model(model, test_loader)\n",
        "\n",
        "    pooling_results.append({\n",
        "        'pooling': pooling,\n",
        "        'train_losses': train_losses,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3pDhIzV8-f3",
        "outputId": "49deb1d0-df2c-4117-c00a-f29511ef107c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Pooling Method: max\n",
            "Epoch 1/50, Loss: 544.1532\n",
            "Epoch 2/50, Loss: 543.9783\n",
            "Epoch 3/50, Loss: 543.9233\n",
            "Epoch 4/50, Loss: 543.8471\n",
            "Epoch 5/50, Loss: 543.9037\n",
            "Epoch 6/50, Loss: 543.8684\n",
            "Epoch 7/50, Loss: 543.8751\n",
            "Epoch 8/50, Loss: 543.8215\n",
            "Epoch 9/50, Loss: 543.8635\n",
            "Epoch 10/50, Loss: 543.8293\n",
            "Epoch 11/50, Loss: 543.8523\n",
            "Epoch 12/50, Loss: 543.8376\n",
            "Epoch 13/50, Loss: 543.8639\n",
            "Epoch 14/50, Loss: 543.7895\n",
            "Epoch 15/50, Loss: 543.8590\n",
            "Epoch 16/50, Loss: 543.7986\n",
            "Epoch 17/50, Loss: 543.8316\n",
            "Epoch 18/50, Loss: 549.6956\n",
            "Epoch 19/50, Loss: 549.7334\n",
            "Epoch 20/50, Loss: 543.8167\n",
            "Epoch 21/50, Loss: 549.7029\n",
            "Epoch 22/50, Loss: 543.8029\n",
            "Epoch 23/50, Loss: 555.5713\n",
            "Epoch 24/50, Loss: 543.8219\n",
            "Epoch 25/50, Loss: 543.8374\n",
            "Epoch 26/50, Loss: 543.8331\n",
            "Epoch 27/50, Loss: 543.8178\n",
            "Epoch 28/50, Loss: 543.8316\n",
            "Epoch 29/50, Loss: 549.7506\n",
            "Epoch 30/50, Loss: 543.8209\n",
            "Epoch 31/50, Loss: 543.8150\n",
            "Epoch 32/50, Loss: 543.8023\n",
            "Epoch 33/50, Loss: 549.6919\n",
            "Epoch 34/50, Loss: 543.8438\n",
            "Epoch 35/50, Loss: 543.8497\n",
            "Epoch 36/50, Loss: 543.8192\n",
            "Epoch 37/50, Loss: 543.7951\n",
            "Epoch 38/50, Loss: 543.7827\n",
            "Epoch 39/50, Loss: 543.8042\n",
            "Epoch 40/50, Loss: 543.8496\n",
            "Epoch 41/50, Loss: 543.8190\n",
            "Epoch 42/50, Loss: 549.6896\n",
            "Epoch 43/50, Loss: 549.7351\n",
            "Epoch 44/50, Loss: 543.8001\n",
            "Epoch 45/50, Loss: 543.8043\n",
            "Epoch 46/50, Loss: 543.8061\n",
            "Epoch 47/50, Loss: 543.7890\n",
            "Epoch 48/50, Loss: 543.7912\n",
            "Epoch 49/50, Loss: 543.7812\n",
            "Epoch 50/50, Loss: 549.6817\n",
            "\n",
            "Training with Pooling Method: avg\n",
            "Epoch 1/50, Loss: 544.0801\n",
            "Epoch 2/50, Loss: 543.9793\n",
            "Epoch 3/50, Loss: 543.9357\n",
            "Epoch 4/50, Loss: 543.8611\n",
            "Epoch 5/50, Loss: 543.8551\n",
            "Epoch 6/50, Loss: 543.8903\n",
            "Epoch 7/50, Loss: 543.8774\n",
            "Epoch 8/50, Loss: 549.7375\n",
            "Epoch 9/50, Loss: 543.8768\n",
            "Epoch 10/50, Loss: 543.8247\n",
            "Epoch 11/50, Loss: 543.8577\n",
            "Epoch 12/50, Loss: 543.8201\n",
            "Epoch 13/50, Loss: 543.8970\n",
            "Epoch 14/50, Loss: 549.7025\n",
            "Epoch 15/50, Loss: 543.8418\n",
            "Epoch 16/50, Loss: 543.8451\n",
            "Epoch 17/50, Loss: 543.8037\n",
            "Epoch 18/50, Loss: 543.8023\n",
            "Epoch 19/50, Loss: 543.8372\n",
            "Epoch 20/50, Loss: 543.7967\n",
            "Epoch 21/50, Loss: 549.7286\n",
            "Epoch 22/50, Loss: 543.8350\n",
            "Epoch 23/50, Loss: 543.7947\n",
            "Epoch 24/50, Loss: 543.8097\n",
            "Epoch 25/50, Loss: 549.7038\n",
            "Epoch 26/50, Loss: 543.8303\n",
            "Epoch 27/50, Loss: 543.8098\n",
            "Epoch 28/50, Loss: 543.7957\n",
            "Epoch 29/50, Loss: 543.7965\n",
            "Epoch 30/50, Loss: 549.6933\n",
            "Epoch 31/50, Loss: 543.8068\n",
            "Epoch 32/50, Loss: 543.8969\n",
            "Epoch 33/50, Loss: 543.7902\n",
            "Epoch 34/50, Loss: 549.7173\n",
            "Epoch 35/50, Loss: 543.7966\n",
            "Epoch 36/50, Loss: 543.8268\n",
            "Epoch 37/50, Loss: 543.7988\n",
            "Epoch 38/50, Loss: 543.7839\n",
            "Epoch 39/50, Loss: 543.7942\n",
            "Epoch 40/50, Loss: 543.7886\n",
            "Epoch 41/50, Loss: 543.8786\n",
            "Epoch 42/50, Loss: 543.8124\n",
            "Epoch 43/50, Loss: 549.7181\n",
            "Epoch 44/50, Loss: 543.8069\n",
            "Epoch 45/50, Loss: 549.6964\n",
            "Epoch 46/50, Loss: 543.7894\n",
            "Epoch 47/50, Loss: 543.7892\n",
            "Epoch 48/50, Loss: 543.8026\n",
            "Epoch 49/50, Loss: 543.7917\n",
            "Epoch 50/50, Loss: 543.8148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment by Epochs\n",
        "epoch_results = []\n",
        "for num_epochs in num_epochs_list:\n",
        "    print(f\"\\nTraining with Epochs: {num_epochs}\")\n",
        "    model = RNNModel(input_size, 64, output_size).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_losses = train_model(model, train_loader, criterion, optimizer, num_epochs=num_epochs)\n",
        "    predictions, actuals = evaluate_model(model, test_loader)\n",
        "\n",
        "    epoch_results.append({\n",
        "        'num_epochs': num_epochs,\n",
        "        'train_losses': train_losses,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOjooyvb9C3R",
        "outputId": "c9297801-fcd5-4828-8692-ec8cf0bbf773"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Epochs: 5\n",
            "Epoch 1/5, Loss: 543.9992\n",
            "Epoch 2/5, Loss: 543.9518\n",
            "Epoch 3/5, Loss: 543.9458\n",
            "Epoch 4/5, Loss: 543.8332\n",
            "Epoch 5/5, Loss: 549.7256\n",
            "\n",
            "Training with Epochs: 50\n",
            "Epoch 1/50, Loss: 549.9702\n",
            "Epoch 2/50, Loss: 549.8713\n",
            "Epoch 3/50, Loss: 549.7652\n",
            "Epoch 4/50, Loss: 543.8565\n",
            "Epoch 5/50, Loss: 543.9096\n",
            "Epoch 6/50, Loss: 543.8428\n",
            "Epoch 7/50, Loss: 543.8900\n",
            "Epoch 8/50, Loss: 543.8818\n",
            "Epoch 9/50, Loss: 543.8413\n",
            "Epoch 10/50, Loss: 543.8485\n",
            "Epoch 11/50, Loss: 543.8119\n",
            "Epoch 12/50, Loss: 543.8598\n",
            "Epoch 13/50, Loss: 543.8039\n",
            "Epoch 14/50, Loss: 543.8503\n",
            "Epoch 15/50, Loss: 543.8571\n",
            "Epoch 16/50, Loss: 543.8786\n",
            "Epoch 17/50, Loss: 543.8119\n",
            "Epoch 18/50, Loss: 543.8171\n",
            "Epoch 19/50, Loss: 543.8400\n",
            "Epoch 20/50, Loss: 543.8110\n",
            "Epoch 21/50, Loss: 543.7967\n",
            "Epoch 22/50, Loss: 543.8134\n",
            "Epoch 23/50, Loss: 543.8034\n",
            "Epoch 24/50, Loss: 555.6179\n",
            "Epoch 25/50, Loss: 543.8386\n",
            "Epoch 26/50, Loss: 549.6946\n",
            "Epoch 27/50, Loss: 543.8584\n",
            "Epoch 28/50, Loss: 543.8086\n",
            "Epoch 29/50, Loss: 549.7146\n",
            "Epoch 30/50, Loss: 543.8045\n",
            "Epoch 31/50, Loss: 549.6840\n",
            "Epoch 32/50, Loss: 543.8072\n",
            "Epoch 33/50, Loss: 543.7858\n",
            "Epoch 34/50, Loss: 543.7941\n",
            "Epoch 35/50, Loss: 543.7786\n",
            "Epoch 36/50, Loss: 543.7886\n",
            "Epoch 37/50, Loss: 543.7872\n",
            "Epoch 38/50, Loss: 543.8064\n",
            "Epoch 39/50, Loss: 543.7787\n",
            "Epoch 40/50, Loss: 543.8151\n",
            "Epoch 41/50, Loss: 543.7796\n",
            "Epoch 42/50, Loss: 543.7708\n",
            "Epoch 43/50, Loss: 543.7912\n",
            "Epoch 44/50, Loss: 543.8075\n",
            "Epoch 45/50, Loss: 543.7732\n",
            "Epoch 46/50, Loss: 543.8153\n",
            "Epoch 47/50, Loss: 543.7922\n",
            "Epoch 48/50, Loss: 543.7890\n",
            "Epoch 49/50, Loss: 543.7916\n",
            "Epoch 50/50, Loss: 543.7948\n",
            "\n",
            "Training with Epochs: 100\n",
            "Epoch 1/100, Loss: 544.0828\n",
            "Epoch 2/100, Loss: 549.8817\n",
            "Epoch 3/100, Loss: 543.8594\n",
            "Epoch 4/100, Loss: 543.8429\n",
            "Epoch 5/100, Loss: 543.8806\n",
            "Epoch 6/100, Loss: 543.8517\n",
            "Epoch 7/100, Loss: 543.8837\n",
            "Epoch 8/100, Loss: 543.8481\n",
            "Epoch 9/100, Loss: 543.8705\n",
            "Epoch 10/100, Loss: 543.8282\n",
            "Epoch 11/100, Loss: 543.7839\n",
            "Epoch 12/100, Loss: 549.7147\n",
            "Epoch 13/100, Loss: 543.8412\n",
            "Epoch 14/100, Loss: 543.8047\n",
            "Epoch 15/100, Loss: 543.8200\n",
            "Epoch 16/100, Loss: 543.8049\n",
            "Epoch 17/100, Loss: 543.8384\n",
            "Epoch 18/100, Loss: 549.7117\n",
            "Epoch 19/100, Loss: 543.8126\n",
            "Epoch 20/100, Loss: 543.8174\n",
            "Epoch 21/100, Loss: 543.7808\n",
            "Epoch 22/100, Loss: 543.8270\n",
            "Epoch 23/100, Loss: 549.6990\n",
            "Epoch 24/100, Loss: 543.8014\n",
            "Epoch 25/100, Loss: 543.8137\n",
            "Epoch 26/100, Loss: 543.7742\n",
            "Epoch 27/100, Loss: 543.8521\n",
            "Epoch 28/100, Loss: 543.7712\n",
            "Epoch 29/100, Loss: 555.6265\n",
            "Epoch 30/100, Loss: 543.7942\n",
            "Epoch 31/100, Loss: 543.7785\n",
            "Epoch 32/100, Loss: 549.7349\n",
            "Epoch 33/100, Loss: 543.7729\n",
            "Epoch 34/100, Loss: 543.7890\n",
            "Epoch 35/100, Loss: 543.7765\n",
            "Epoch 36/100, Loss: 543.7790\n",
            "Epoch 37/100, Loss: 543.7938\n",
            "Epoch 38/100, Loss: 543.7705\n",
            "Epoch 39/100, Loss: 543.7890\n",
            "Epoch 40/100, Loss: 543.8351\n",
            "Epoch 41/100, Loss: 543.8149\n",
            "Epoch 42/100, Loss: 543.7710\n",
            "Epoch 43/100, Loss: 543.7937\n",
            "Epoch 44/100, Loss: 549.6916\n",
            "Epoch 45/100, Loss: 543.7916\n",
            "Epoch 46/100, Loss: 543.7969\n",
            "Epoch 47/100, Loss: 543.7942\n",
            "Epoch 48/100, Loss: 543.7879\n",
            "Epoch 49/100, Loss: 549.6990\n",
            "Epoch 50/100, Loss: 543.7972\n",
            "Epoch 51/100, Loss: 543.7786\n",
            "Epoch 52/100, Loss: 543.7768\n",
            "Epoch 53/100, Loss: 549.6708\n",
            "Epoch 54/100, Loss: 543.7974\n",
            "Epoch 55/100, Loss: 543.7764\n",
            "Epoch 56/100, Loss: 549.6881\n",
            "Epoch 57/100, Loss: 549.6588\n",
            "Epoch 58/100, Loss: 543.7967\n",
            "Epoch 59/100, Loss: 543.7979\n",
            "Epoch 60/100, Loss: 543.8040\n",
            "Epoch 61/100, Loss: 549.6847\n",
            "Epoch 62/100, Loss: 549.6713\n",
            "Epoch 63/100, Loss: 549.6729\n",
            "Epoch 64/100, Loss: 543.7618\n",
            "Epoch 65/100, Loss: 543.7900\n",
            "Epoch 66/100, Loss: 549.6771\n",
            "Epoch 67/100, Loss: 543.7816\n",
            "Epoch 68/100, Loss: 543.7746\n",
            "Epoch 69/100, Loss: 543.7658\n",
            "Epoch 70/100, Loss: 543.8037\n",
            "Epoch 71/100, Loss: 543.7744\n",
            "Epoch 72/100, Loss: 543.7520\n",
            "Epoch 73/100, Loss: 543.7636\n",
            "Epoch 74/100, Loss: 543.7719\n",
            "Epoch 75/100, Loss: 543.7589\n",
            "Epoch 76/100, Loss: 549.6572\n",
            "Epoch 77/100, Loss: 555.5530\n",
            "Epoch 78/100, Loss: 543.7579\n",
            "Epoch 79/100, Loss: 543.8008\n",
            "Epoch 80/100, Loss: 543.7755\n",
            "Epoch 81/100, Loss: 549.6601\n",
            "Epoch 82/100, Loss: 549.6657\n",
            "Epoch 83/100, Loss: 549.6745\n",
            "Epoch 84/100, Loss: 543.7866\n",
            "Epoch 85/100, Loss: 543.7689\n",
            "Epoch 86/100, Loss: 543.7844\n",
            "Epoch 87/100, Loss: 543.7659\n",
            "Epoch 88/100, Loss: 543.7700\n",
            "Epoch 89/100, Loss: 543.7652\n",
            "Epoch 90/100, Loss: 549.6703\n",
            "Epoch 91/100, Loss: 543.7441\n",
            "Epoch 92/100, Loss: 543.9577\n",
            "Epoch 93/100, Loss: 543.7792\n",
            "Epoch 94/100, Loss: 543.8041\n",
            "Epoch 95/100, Loss: 543.8582\n",
            "Epoch 96/100, Loss: 543.8136\n",
            "Epoch 97/100, Loss: 543.8934\n",
            "Epoch 98/100, Loss: 549.7244\n",
            "Epoch 99/100, Loss: 549.7102\n",
            "Epoch 100/100, Loss: 543.8020\n",
            "\n",
            "Training with Epochs: 250\n",
            "Epoch 1/250, Loss: 544.0321\n",
            "Epoch 2/250, Loss: 543.8809\n",
            "Epoch 3/250, Loss: 543.8459\n",
            "Epoch 4/250, Loss: 543.8528\n",
            "Epoch 5/250, Loss: 549.7703\n",
            "Epoch 6/250, Loss: 543.8713\n",
            "Epoch 7/250, Loss: 543.8611\n",
            "Epoch 8/250, Loss: 549.7109\n",
            "Epoch 9/250, Loss: 543.8108\n",
            "Epoch 10/250, Loss: 543.8054\n",
            "Epoch 11/250, Loss: 549.7079\n",
            "Epoch 12/250, Loss: 543.8547\n",
            "Epoch 13/250, Loss: 543.8062\n",
            "Epoch 14/250, Loss: 543.7882\n",
            "Epoch 15/250, Loss: 543.8216\n",
            "Epoch 16/250, Loss: 543.8331\n",
            "Epoch 17/250, Loss: 543.8017\n",
            "Epoch 18/250, Loss: 543.8075\n",
            "Epoch 19/250, Loss: 543.7947\n",
            "Epoch 20/250, Loss: 543.8054\n",
            "Epoch 21/250, Loss: 543.7892\n",
            "Epoch 22/250, Loss: 543.8013\n",
            "Epoch 23/250, Loss: 543.7851\n",
            "Epoch 24/250, Loss: 543.8405\n",
            "Epoch 25/250, Loss: 543.7625\n",
            "Epoch 26/250, Loss: 543.7817\n",
            "Epoch 27/250, Loss: 549.6990\n",
            "Epoch 28/250, Loss: 543.8203\n",
            "Epoch 29/250, Loss: 543.7904\n",
            "Epoch 30/250, Loss: 543.8137\n",
            "Epoch 31/250, Loss: 543.7885\n",
            "Epoch 32/250, Loss: 543.7925\n",
            "Epoch 33/250, Loss: 549.7079\n",
            "Epoch 34/250, Loss: 549.6977\n",
            "Epoch 35/250, Loss: 543.8201\n",
            "Epoch 36/250, Loss: 543.7829\n",
            "Epoch 37/250, Loss: 543.8484\n",
            "Epoch 38/250, Loss: 543.7890\n",
            "Epoch 39/250, Loss: 543.7844\n",
            "Epoch 40/250, Loss: 543.7904\n",
            "Epoch 41/250, Loss: 543.7585\n",
            "Epoch 42/250, Loss: 549.8168\n",
            "Epoch 43/250, Loss: 549.7037\n",
            "Epoch 44/250, Loss: 549.6766\n",
            "Epoch 45/250, Loss: 543.7854\n",
            "Epoch 46/250, Loss: 549.6506\n",
            "Epoch 47/250, Loss: 549.6902\n",
            "Epoch 48/250, Loss: 543.7762\n",
            "Epoch 49/250, Loss: 543.7652\n",
            "Epoch 50/250, Loss: 543.7960\n",
            "Epoch 51/250, Loss: 543.7978\n",
            "Epoch 52/250, Loss: 543.7679\n",
            "Epoch 53/250, Loss: 543.7847\n",
            "Epoch 54/250, Loss: 543.7807\n",
            "Epoch 55/250, Loss: 543.7680\n",
            "Epoch 56/250, Loss: 543.7565\n",
            "Epoch 57/250, Loss: 549.6872\n",
            "Epoch 58/250, Loss: 543.7688\n",
            "Epoch 59/250, Loss: 543.7745\n",
            "Epoch 60/250, Loss: 543.7735\n",
            "Epoch 61/250, Loss: 543.8003\n",
            "Epoch 62/250, Loss: 543.8000\n",
            "Epoch 63/250, Loss: 543.7782\n",
            "Epoch 64/250, Loss: 543.7797\n",
            "Epoch 65/250, Loss: 543.7469\n",
            "Epoch 66/250, Loss: 543.7951\n",
            "Epoch 67/250, Loss: 549.6697\n",
            "Epoch 68/250, Loss: 543.7655\n",
            "Epoch 69/250, Loss: 543.8142\n",
            "Epoch 70/250, Loss: 543.7700\n",
            "Epoch 71/250, Loss: 543.7588\n",
            "Epoch 72/250, Loss: 543.7597\n",
            "Epoch 73/250, Loss: 549.6588\n",
            "Epoch 74/250, Loss: 543.7770\n",
            "Epoch 75/250, Loss: 555.5753\n",
            "Epoch 76/250, Loss: 543.7880\n",
            "Epoch 77/250, Loss: 543.7598\n",
            "Epoch 78/250, Loss: 549.6735\n",
            "Epoch 79/250, Loss: 543.7650\n",
            "Epoch 80/250, Loss: 543.7535\n",
            "Epoch 81/250, Loss: 543.7646\n",
            "Epoch 82/250, Loss: 543.7844\n",
            "Epoch 83/250, Loss: 543.7426\n",
            "Epoch 84/250, Loss: 543.9011\n",
            "Epoch 85/250, Loss: 549.7015\n",
            "Epoch 86/250, Loss: 543.7672\n",
            "Epoch 87/250, Loss: 543.7618\n",
            "Epoch 88/250, Loss: 543.7743\n",
            "Epoch 89/250, Loss: 549.6468\n",
            "Epoch 90/250, Loss: 543.7667\n",
            "Epoch 91/250, Loss: 543.7727\n",
            "Epoch 92/250, Loss: 543.7709\n",
            "Epoch 93/250, Loss: 543.7706\n",
            "Epoch 94/250, Loss: 543.7506\n",
            "Epoch 95/250, Loss: 543.7630\n",
            "Epoch 96/250, Loss: 543.7804\n",
            "Epoch 97/250, Loss: 543.8023\n",
            "Epoch 98/250, Loss: 543.7553\n",
            "Epoch 99/250, Loss: 549.6856\n",
            "Epoch 100/250, Loss: 543.7765\n",
            "Epoch 101/250, Loss: 543.7818\n",
            "Epoch 102/250, Loss: 543.7484\n",
            "Epoch 103/250, Loss: 549.6510\n",
            "Epoch 104/250, Loss: 543.7719\n",
            "Epoch 105/250, Loss: 543.7855\n",
            "Epoch 106/250, Loss: 543.7604\n",
            "Epoch 107/250, Loss: 543.7519\n",
            "Epoch 108/250, Loss: 549.6771\n",
            "Epoch 109/250, Loss: 543.7614\n",
            "Epoch 110/250, Loss: 543.7522\n",
            "Epoch 111/250, Loss: 543.7724\n",
            "Epoch 112/250, Loss: 549.6751\n",
            "Epoch 113/250, Loss: 543.7562\n",
            "Epoch 114/250, Loss: 543.7541\n",
            "Epoch 115/250, Loss: 543.7887\n",
            "Epoch 116/250, Loss: 543.7583\n",
            "Epoch 117/250, Loss: 543.7720\n",
            "Epoch 118/250, Loss: 543.7649\n",
            "Epoch 119/250, Loss: 543.7702\n",
            "Epoch 120/250, Loss: 543.7532\n",
            "Epoch 121/250, Loss: 543.7655\n",
            "Epoch 122/250, Loss: 543.7981\n",
            "Epoch 123/250, Loss: 543.7457\n",
            "Epoch 124/250, Loss: 543.7512\n",
            "Epoch 125/250, Loss: 543.7923\n",
            "Epoch 126/250, Loss: 549.6750\n",
            "Epoch 127/250, Loss: 543.7749\n",
            "Epoch 128/250, Loss: 543.7635\n",
            "Epoch 129/250, Loss: 543.7824\n",
            "Epoch 130/250, Loss: 543.7717\n",
            "Epoch 131/250, Loss: 543.7654\n",
            "Epoch 132/250, Loss: 543.7435\n",
            "Epoch 133/250, Loss: 543.7781\n",
            "Epoch 134/250, Loss: 555.5597\n",
            "Epoch 135/250, Loss: 543.7640\n",
            "Epoch 136/250, Loss: 543.7683\n",
            "Epoch 137/250, Loss: 543.7609\n",
            "Epoch 138/250, Loss: 543.7666\n",
            "Epoch 139/250, Loss: 543.7896\n",
            "Epoch 140/250, Loss: 543.7783\n",
            "Epoch 141/250, Loss: 543.7736\n",
            "Epoch 142/250, Loss: 543.7688\n",
            "Epoch 143/250, Loss: 543.7441\n",
            "Epoch 144/250, Loss: 543.7727\n",
            "Epoch 145/250, Loss: 549.6832\n",
            "Epoch 146/250, Loss: 543.7816\n",
            "Epoch 147/250, Loss: 543.7619\n",
            "Epoch 148/250, Loss: 543.7547\n",
            "Epoch 149/250, Loss: 543.7716\n",
            "Epoch 150/250, Loss: 543.7931\n",
            "Epoch 151/250, Loss: 543.7826\n",
            "Epoch 152/250, Loss: 555.5616\n",
            "Epoch 153/250, Loss: 543.7548\n",
            "Epoch 154/250, Loss: 543.8133\n",
            "Epoch 155/250, Loss: 549.6420\n",
            "Epoch 156/250, Loss: 543.8031\n",
            "Epoch 157/250, Loss: 543.7824\n",
            "Epoch 158/250, Loss: 555.5561\n",
            "Epoch 159/250, Loss: 543.7494\n",
            "Epoch 160/250, Loss: 543.7626\n",
            "Epoch 161/250, Loss: 543.7370\n",
            "Epoch 162/250, Loss: 543.7693\n",
            "Epoch 163/250, Loss: 549.6482\n",
            "Epoch 164/250, Loss: 543.7606\n",
            "Epoch 165/250, Loss: 549.6839\n",
            "Epoch 166/250, Loss: 543.7749\n",
            "Epoch 167/250, Loss: 543.7683\n",
            "Epoch 168/250, Loss: 543.7697\n",
            "Epoch 169/250, Loss: 543.7771\n",
            "Epoch 170/250, Loss: 543.7856\n",
            "Epoch 171/250, Loss: 543.7606\n",
            "Epoch 172/250, Loss: 543.7650\n",
            "Epoch 173/250, Loss: 549.6719\n",
            "Epoch 174/250, Loss: 543.7520\n",
            "Epoch 175/250, Loss: 549.6740\n",
            "Epoch 176/250, Loss: 543.7596\n",
            "Epoch 177/250, Loss: 543.7756\n",
            "Epoch 178/250, Loss: 543.7904\n",
            "Epoch 179/250, Loss: 543.7707\n",
            "Epoch 180/250, Loss: 543.7638\n",
            "Epoch 181/250, Loss: 543.7694\n",
            "Epoch 182/250, Loss: 543.7796\n",
            "Epoch 183/250, Loss: 543.7763\n",
            "Epoch 184/250, Loss: 543.7652\n",
            "Epoch 185/250, Loss: 543.7644\n",
            "Epoch 186/250, Loss: 543.7570\n",
            "Epoch 187/250, Loss: 543.7670\n",
            "Epoch 188/250, Loss: 543.7548\n",
            "Epoch 189/250, Loss: 543.7638\n",
            "Epoch 190/250, Loss: 543.7876\n",
            "Epoch 191/250, Loss: 543.7432\n",
            "Epoch 192/250, Loss: 543.7764\n",
            "Epoch 193/250, Loss: 543.7817\n",
            "Epoch 194/250, Loss: 543.7809\n",
            "Epoch 195/250, Loss: 543.7679\n",
            "Epoch 196/250, Loss: 549.6618\n",
            "Epoch 197/250, Loss: 543.7585\n",
            "Epoch 198/250, Loss: 543.7660\n",
            "Epoch 199/250, Loss: 543.7757\n",
            "Epoch 200/250, Loss: 543.7825\n",
            "Epoch 201/250, Loss: 549.6552\n",
            "Epoch 202/250, Loss: 549.6545\n",
            "Epoch 203/250, Loss: 549.6584\n",
            "Epoch 204/250, Loss: 543.7684\n",
            "Epoch 205/250, Loss: 543.7780\n",
            "Epoch 206/250, Loss: 543.7685\n",
            "Epoch 207/250, Loss: 543.7697\n",
            "Epoch 208/250, Loss: 543.7733\n",
            "Epoch 209/250, Loss: 543.7683\n",
            "Epoch 210/250, Loss: 543.7686\n",
            "Epoch 211/250, Loss: 543.7572\n",
            "Epoch 212/250, Loss: 543.7687\n",
            "Epoch 213/250, Loss: 549.6497\n",
            "Epoch 214/250, Loss: 543.7664\n",
            "Epoch 215/250, Loss: 549.6735\n",
            "Epoch 216/250, Loss: 543.7527\n",
            "Epoch 217/250, Loss: 543.7749\n",
            "Epoch 218/250, Loss: 543.7526\n",
            "Epoch 219/250, Loss: 543.7532\n",
            "Epoch 220/250, Loss: 543.7656\n",
            "Epoch 221/250, Loss: 543.7744\n",
            "Epoch 222/250, Loss: 543.7730\n",
            "Epoch 223/250, Loss: 549.6548\n",
            "Epoch 224/250, Loss: 543.7874\n",
            "Epoch 225/250, Loss: 543.7597\n",
            "Epoch 226/250, Loss: 543.7948\n",
            "Epoch 227/250, Loss: 543.7550\n",
            "Epoch 228/250, Loss: 549.6669\n",
            "Epoch 229/250, Loss: 543.7502\n",
            "Epoch 230/250, Loss: 543.8017\n",
            "Epoch 231/250, Loss: 543.7662\n",
            "Epoch 232/250, Loss: 543.7573\n",
            "Epoch 233/250, Loss: 543.7603\n",
            "Epoch 234/250, Loss: 549.6688\n",
            "Epoch 235/250, Loss: 543.7514\n",
            "Epoch 236/250, Loss: 543.7713\n",
            "Epoch 237/250, Loss: 543.7658\n",
            "Epoch 238/250, Loss: 543.7671\n",
            "Epoch 239/250, Loss: 543.7859\n",
            "Epoch 240/250, Loss: 543.7607\n",
            "Epoch 241/250, Loss: 543.7669\n",
            "Epoch 242/250, Loss: 543.7783\n",
            "Epoch 243/250, Loss: 543.7818\n",
            "Epoch 244/250, Loss: 543.7549\n",
            "Epoch 245/250, Loss: 543.7656\n",
            "Epoch 246/250, Loss: 543.7790\n",
            "Epoch 247/250, Loss: 543.7710\n",
            "Epoch 248/250, Loss: 549.6677\n",
            "Epoch 249/250, Loss: 543.7588\n",
            "Epoch 250/250, Loss: 543.7808\n",
            "\n",
            "Training with Epochs: 350\n",
            "Epoch 1/350, Loss: 550.0271\n",
            "Epoch 2/350, Loss: 543.8704\n",
            "Epoch 3/350, Loss: 543.8512\n",
            "Epoch 4/350, Loss: 549.7705\n",
            "Epoch 5/350, Loss: 543.8504\n",
            "Epoch 6/350, Loss: 543.8503\n",
            "Epoch 7/350, Loss: 543.8250\n",
            "Epoch 8/350, Loss: 543.8651\n",
            "Epoch 9/350, Loss: 543.8420\n",
            "Epoch 10/350, Loss: 543.8383\n",
            "Epoch 11/350, Loss: 543.8074\n",
            "Epoch 12/350, Loss: 543.7892\n",
            "Epoch 13/350, Loss: 543.8008\n",
            "Epoch 14/350, Loss: 543.8134\n",
            "Epoch 15/350, Loss: 543.8278\n",
            "Epoch 16/350, Loss: 549.7296\n",
            "Epoch 17/350, Loss: 543.7813\n",
            "Epoch 18/350, Loss: 543.7877\n",
            "Epoch 19/350, Loss: 543.8445\n",
            "Epoch 20/350, Loss: 543.7938\n",
            "Epoch 21/350, Loss: 543.8324\n",
            "Epoch 22/350, Loss: 555.6062\n",
            "Epoch 23/350, Loss: 543.7954\n",
            "Epoch 24/350, Loss: 543.8481\n",
            "Epoch 25/350, Loss: 543.7894\n",
            "Epoch 26/350, Loss: 543.7975\n",
            "Epoch 27/350, Loss: 543.8244\n",
            "Epoch 28/350, Loss: 543.8479\n",
            "Epoch 29/350, Loss: 543.8099\n",
            "Epoch 30/350, Loss: 543.7984\n",
            "Epoch 31/350, Loss: 543.7914\n",
            "Epoch 32/350, Loss: 543.7885\n",
            "Epoch 33/350, Loss: 543.7719\n",
            "Epoch 34/350, Loss: 543.7732\n",
            "Epoch 35/350, Loss: 543.8135\n",
            "Epoch 36/350, Loss: 543.8335\n",
            "Epoch 37/350, Loss: 549.6918\n",
            "Epoch 38/350, Loss: 543.7722\n",
            "Epoch 39/350, Loss: 543.7890\n",
            "Epoch 40/350, Loss: 543.8012\n",
            "Epoch 41/350, Loss: 549.6674\n",
            "Epoch 42/350, Loss: 543.7914\n",
            "Epoch 43/350, Loss: 543.7863\n",
            "Epoch 44/350, Loss: 543.7867\n",
            "Epoch 45/350, Loss: 543.8125\n",
            "Epoch 46/350, Loss: 543.8035\n",
            "Epoch 47/350, Loss: 543.7994\n",
            "Epoch 48/350, Loss: 549.6881\n",
            "Epoch 49/350, Loss: 543.7551\n",
            "Epoch 50/350, Loss: 543.7908\n",
            "Epoch 51/350, Loss: 543.7923\n",
            "Epoch 52/350, Loss: 543.7700\n",
            "Epoch 53/350, Loss: 543.7870\n",
            "Epoch 54/350, Loss: 543.7521\n",
            "Epoch 55/350, Loss: 543.7721\n",
            "Epoch 56/350, Loss: 543.8002\n",
            "Epoch 57/350, Loss: 543.8235\n",
            "Epoch 58/350, Loss: 543.7583\n",
            "Epoch 59/350, Loss: 543.7945\n",
            "Epoch 60/350, Loss: 543.7608\n",
            "Epoch 61/350, Loss: 543.7563\n",
            "Epoch 62/350, Loss: 543.8566\n",
            "Epoch 63/350, Loss: 543.7576\n",
            "Epoch 64/350, Loss: 543.7848\n",
            "Epoch 65/350, Loss: 543.7787\n",
            "Epoch 66/350, Loss: 543.8078\n",
            "Epoch 67/350, Loss: 543.7779\n",
            "Epoch 68/350, Loss: 543.7718\n",
            "Epoch 69/350, Loss: 543.7469\n",
            "Epoch 70/350, Loss: 543.7841\n",
            "Epoch 71/350, Loss: 543.7737\n",
            "Epoch 72/350, Loss: 543.7817\n",
            "Epoch 73/350, Loss: 543.7726\n",
            "Epoch 74/350, Loss: 543.7842\n",
            "Epoch 75/350, Loss: 543.7835\n",
            "Epoch 76/350, Loss: 543.7799\n",
            "Epoch 77/350, Loss: 549.6790\n",
            "Epoch 78/350, Loss: 543.7713\n",
            "Epoch 79/350, Loss: 543.7654\n",
            "Epoch 80/350, Loss: 543.7744\n",
            "Epoch 81/350, Loss: 543.7880\n",
            "Epoch 82/350, Loss: 543.7917\n",
            "Epoch 83/350, Loss: 543.7830\n",
            "Epoch 84/350, Loss: 543.7908\n",
            "Epoch 85/350, Loss: 543.7528\n",
            "Epoch 86/350, Loss: 543.7822\n",
            "Epoch 87/350, Loss: 543.7734\n",
            "Epoch 88/350, Loss: 543.7634\n",
            "Epoch 89/350, Loss: 543.7828\n",
            "Epoch 90/350, Loss: 549.6812\n",
            "Epoch 91/350, Loss: 543.7784\n",
            "Epoch 92/350, Loss: 543.7778\n",
            "Epoch 93/350, Loss: 543.7624\n",
            "Epoch 94/350, Loss: 543.7567\n",
            "Epoch 95/350, Loss: 543.7596\n",
            "Epoch 96/350, Loss: 549.6845\n",
            "Epoch 97/350, Loss: 543.7788\n",
            "Epoch 98/350, Loss: 543.7718\n",
            "Epoch 99/350, Loss: 543.7845\n",
            "Epoch 100/350, Loss: 543.7936\n",
            "Epoch 101/350, Loss: 549.6460\n",
            "Epoch 102/350, Loss: 543.7933\n",
            "Epoch 103/350, Loss: 543.8118\n",
            "Epoch 104/350, Loss: 543.8019\n",
            "Epoch 105/350, Loss: 543.7988\n",
            "Epoch 106/350, Loss: 549.6806\n",
            "Epoch 107/350, Loss: 543.7757\n",
            "Epoch 108/350, Loss: 549.6461\n",
            "Epoch 109/350, Loss: 549.6248\n",
            "Epoch 110/350, Loss: 543.7860\n",
            "Epoch 111/350, Loss: 543.7811\n",
            "Epoch 112/350, Loss: 543.7459\n",
            "Epoch 113/350, Loss: 543.7890\n",
            "Epoch 114/350, Loss: 543.7669\n",
            "Epoch 115/350, Loss: 543.7688\n",
            "Epoch 116/350, Loss: 543.8016\n",
            "Epoch 117/350, Loss: 543.8080\n",
            "Epoch 118/350, Loss: 543.7674\n",
            "Epoch 119/350, Loss: 543.7681\n",
            "Epoch 120/350, Loss: 543.7653\n",
            "Epoch 121/350, Loss: 543.7572\n",
            "Epoch 122/350, Loss: 543.7700\n",
            "Epoch 123/350, Loss: 543.7665\n",
            "Epoch 124/350, Loss: 543.7738\n",
            "Epoch 125/350, Loss: 543.7842\n",
            "Epoch 126/350, Loss: 549.6658\n",
            "Epoch 127/350, Loss: 543.7761\n",
            "Epoch 128/350, Loss: 543.7777\n",
            "Epoch 129/350, Loss: 549.6538\n",
            "Epoch 130/350, Loss: 549.6805\n",
            "Epoch 131/350, Loss: 543.7765\n",
            "Epoch 132/350, Loss: 543.7494\n",
            "Epoch 133/350, Loss: 549.6866\n",
            "Epoch 134/350, Loss: 543.7558\n",
            "Epoch 135/350, Loss: 543.7668\n",
            "Epoch 136/350, Loss: 555.5573\n",
            "Epoch 137/350, Loss: 543.7648\n",
            "Epoch 138/350, Loss: 543.7914\n",
            "Epoch 139/350, Loss: 543.7585\n",
            "Epoch 140/350, Loss: 549.6752\n",
            "Epoch 141/350, Loss: 549.6848\n",
            "Epoch 142/350, Loss: 543.7738\n",
            "Epoch 143/350, Loss: 549.6843\n",
            "Epoch 144/350, Loss: 549.6389\n",
            "Epoch 145/350, Loss: 543.7530\n",
            "Epoch 146/350, Loss: 543.7808\n",
            "Epoch 147/350, Loss: 543.7524\n",
            "Epoch 148/350, Loss: 543.7731\n",
            "Epoch 149/350, Loss: 543.7651\n",
            "Epoch 150/350, Loss: 543.7806\n",
            "Epoch 151/350, Loss: 543.7856\n",
            "Epoch 152/350, Loss: 543.7994\n",
            "Epoch 153/350, Loss: 543.7734\n",
            "Epoch 154/350, Loss: 543.7850\n",
            "Epoch 155/350, Loss: 543.7673\n",
            "Epoch 156/350, Loss: 543.7742\n",
            "Epoch 157/350, Loss: 543.7502\n",
            "Epoch 158/350, Loss: 543.7767\n",
            "Epoch 159/350, Loss: 543.7741\n",
            "Epoch 160/350, Loss: 543.7838\n",
            "Epoch 161/350, Loss: 543.7900\n",
            "Epoch 162/350, Loss: 543.7773\n",
            "Epoch 163/350, Loss: 549.6589\n",
            "Epoch 164/350, Loss: 543.7691\n",
            "Epoch 165/350, Loss: 543.7697\n",
            "Epoch 166/350, Loss: 543.7640\n",
            "Epoch 167/350, Loss: 543.7803\n",
            "Epoch 168/350, Loss: 543.7405\n",
            "Epoch 169/350, Loss: 543.7605\n",
            "Epoch 170/350, Loss: 543.7698\n",
            "Epoch 171/350, Loss: 543.7638\n",
            "Epoch 172/350, Loss: 543.7421\n",
            "Epoch 173/350, Loss: 543.7651\n",
            "Epoch 174/350, Loss: 549.6762\n",
            "Epoch 175/350, Loss: 543.7705\n",
            "Epoch 176/350, Loss: 543.7571\n",
            "Epoch 177/350, Loss: 543.7642\n",
            "Epoch 178/350, Loss: 543.7687\n",
            "Epoch 179/350, Loss: 549.6507\n",
            "Epoch 180/350, Loss: 543.7762\n",
            "Epoch 181/350, Loss: 543.7528\n",
            "Epoch 182/350, Loss: 543.7847\n",
            "Epoch 183/350, Loss: 549.6677\n",
            "Epoch 184/350, Loss: 543.7802\n",
            "Epoch 185/350, Loss: 543.7854\n",
            "Epoch 186/350, Loss: 543.7890\n",
            "Epoch 187/350, Loss: 549.6503\n",
            "Epoch 188/350, Loss: 543.7906\n",
            "Epoch 189/350, Loss: 543.7911\n",
            "Epoch 190/350, Loss: 549.6761\n",
            "Epoch 191/350, Loss: 543.7822\n",
            "Epoch 192/350, Loss: 543.7912\n",
            "Epoch 193/350, Loss: 543.7721\n",
            "Epoch 194/350, Loss: 543.7768\n",
            "Epoch 195/350, Loss: 555.5482\n",
            "Epoch 196/350, Loss: 543.7594\n",
            "Epoch 197/350, Loss: 543.7816\n",
            "Epoch 198/350, Loss: 543.7521\n",
            "Epoch 199/350, Loss: 543.7818\n",
            "Epoch 200/350, Loss: 543.7491\n",
            "Epoch 201/350, Loss: 543.7631\n",
            "Epoch 202/350, Loss: 543.7656\n",
            "Epoch 203/350, Loss: 543.7656\n",
            "Epoch 204/350, Loss: 543.7712\n",
            "Epoch 205/350, Loss: 543.7745\n",
            "Epoch 206/350, Loss: 549.6643\n",
            "Epoch 207/350, Loss: 543.7577\n",
            "Epoch 208/350, Loss: 543.7490\n",
            "Epoch 209/350, Loss: 549.6726\n",
            "Epoch 210/350, Loss: 543.7483\n",
            "Epoch 211/350, Loss: 543.7880\n",
            "Epoch 212/350, Loss: 543.7662\n",
            "Epoch 213/350, Loss: 549.6593\n",
            "Epoch 214/350, Loss: 543.7766\n",
            "Epoch 215/350, Loss: 543.7689\n",
            "Epoch 216/350, Loss: 543.7758\n",
            "Epoch 217/350, Loss: 549.6727\n",
            "Epoch 218/350, Loss: 543.7431\n",
            "Epoch 219/350, Loss: 549.6648\n",
            "Epoch 220/350, Loss: 543.7659\n",
            "Epoch 221/350, Loss: 543.7550\n",
            "Epoch 222/350, Loss: 543.7830\n",
            "Epoch 223/350, Loss: 543.7691\n",
            "Epoch 224/350, Loss: 555.5351\n",
            "Epoch 225/350, Loss: 549.6670\n",
            "Epoch 226/350, Loss: 543.8041\n",
            "Epoch 227/350, Loss: 543.7802\n",
            "Epoch 228/350, Loss: 543.7610\n",
            "Epoch 229/350, Loss: 543.7929\n",
            "Epoch 230/350, Loss: 549.6559\n",
            "Epoch 231/350, Loss: 543.7620\n",
            "Epoch 232/350, Loss: 543.7658\n",
            "Epoch 233/350, Loss: 543.7828\n",
            "Epoch 234/350, Loss: 543.7609\n",
            "Epoch 235/350, Loss: 543.7740\n",
            "Epoch 236/350, Loss: 543.7842\n",
            "Epoch 237/350, Loss: 549.6714\n",
            "Epoch 238/350, Loss: 549.6508\n",
            "Epoch 239/350, Loss: 543.7756\n",
            "Epoch 240/350, Loss: 543.7755\n",
            "Epoch 241/350, Loss: 543.7548\n",
            "Epoch 242/350, Loss: 543.7662\n",
            "Epoch 243/350, Loss: 543.7710\n",
            "Epoch 244/350, Loss: 543.7808\n",
            "Epoch 245/350, Loss: 543.7519\n",
            "Epoch 246/350, Loss: 543.7678\n",
            "Epoch 247/350, Loss: 543.7644\n",
            "Epoch 248/350, Loss: 543.7809\n",
            "Epoch 249/350, Loss: 543.7446\n",
            "Epoch 250/350, Loss: 543.7664\n",
            "Epoch 251/350, Loss: 543.7957\n",
            "Epoch 252/350, Loss: 543.7322\n",
            "Epoch 253/350, Loss: 543.7658\n",
            "Epoch 254/350, Loss: 549.6635\n",
            "Epoch 255/350, Loss: 543.7872\n",
            "Epoch 256/350, Loss: 549.6597\n",
            "Epoch 257/350, Loss: 543.7633\n",
            "Epoch 258/350, Loss: 543.7897\n",
            "Epoch 259/350, Loss: 543.7657\n",
            "Epoch 260/350, Loss: 549.6446\n",
            "Epoch 261/350, Loss: 543.7580\n",
            "Epoch 262/350, Loss: 543.7895\n",
            "Epoch 263/350, Loss: 549.6583\n",
            "Epoch 264/350, Loss: 555.5599\n",
            "Epoch 265/350, Loss: 549.6537\n",
            "Epoch 266/350, Loss: 543.7843\n",
            "Epoch 267/350, Loss: 549.6694\n",
            "Epoch 268/350, Loss: 555.5395\n",
            "Epoch 269/350, Loss: 543.7708\n",
            "Epoch 270/350, Loss: 543.7568\n",
            "Epoch 271/350, Loss: 549.7561\n",
            "Epoch 272/350, Loss: 543.8318\n",
            "Epoch 273/350, Loss: 543.7881\n",
            "Epoch 274/350, Loss: 543.7721\n",
            "Epoch 275/350, Loss: 543.7579\n",
            "Epoch 276/350, Loss: 543.7584\n",
            "Epoch 277/350, Loss: 543.7578\n",
            "Epoch 278/350, Loss: 543.7810\n",
            "Epoch 279/350, Loss: 543.7887\n",
            "Epoch 280/350, Loss: 555.5543\n",
            "Epoch 281/350, Loss: 543.7810\n",
            "Epoch 282/350, Loss: 543.7710\n",
            "Epoch 283/350, Loss: 543.7667\n",
            "Epoch 284/350, Loss: 549.6511\n",
            "Epoch 285/350, Loss: 543.7602\n",
            "Epoch 286/350, Loss: 561.4634\n",
            "Epoch 287/350, Loss: 543.7613\n",
            "Epoch 288/350, Loss: 543.7717\n",
            "Epoch 289/350, Loss: 549.6450\n",
            "Epoch 290/350, Loss: 543.7513\n",
            "Epoch 291/350, Loss: 543.7807\n",
            "Epoch 292/350, Loss: 543.7557\n",
            "Epoch 293/350, Loss: 543.7915\n",
            "Epoch 294/350, Loss: 543.7871\n",
            "Epoch 295/350, Loss: 543.7637\n",
            "Epoch 296/350, Loss: 543.7703\n",
            "Epoch 297/350, Loss: 543.7399\n",
            "Epoch 298/350, Loss: 549.6560\n",
            "Epoch 299/350, Loss: 549.6619\n",
            "Epoch 300/350, Loss: 543.7499\n",
            "Epoch 301/350, Loss: 543.7959\n",
            "Epoch 302/350, Loss: 543.7573\n",
            "Epoch 303/350, Loss: 543.7603\n",
            "Epoch 304/350, Loss: 543.7493\n",
            "Epoch 305/350, Loss: 543.7542\n",
            "Epoch 306/350, Loss: 543.7544\n",
            "Epoch 307/350, Loss: 555.5514\n",
            "Epoch 308/350, Loss: 543.7912\n",
            "Epoch 309/350, Loss: 543.7517\n",
            "Epoch 310/350, Loss: 543.7697\n",
            "Epoch 311/350, Loss: 543.7808\n",
            "Epoch 312/350, Loss: 543.7410\n",
            "Epoch 313/350, Loss: 543.7646\n",
            "Epoch 314/350, Loss: 543.7636\n",
            "Epoch 315/350, Loss: 543.7459\n",
            "Epoch 316/350, Loss: 543.7619\n",
            "Epoch 317/350, Loss: 543.7889\n",
            "Epoch 318/350, Loss: 549.6274\n",
            "Epoch 319/350, Loss: 543.7497\n",
            "Epoch 320/350, Loss: 549.6466\n",
            "Epoch 321/350, Loss: 549.6442\n",
            "Epoch 322/350, Loss: 543.7562\n",
            "Epoch 323/350, Loss: 543.7697\n",
            "Epoch 324/350, Loss: 543.7709\n",
            "Epoch 325/350, Loss: 543.7806\n",
            "Epoch 326/350, Loss: 549.6331\n",
            "Epoch 327/350, Loss: 543.8162\n",
            "Epoch 328/350, Loss: 543.7570\n",
            "Epoch 329/350, Loss: 543.7711\n",
            "Epoch 330/350, Loss: 543.7752\n",
            "Epoch 331/350, Loss: 543.7702\n",
            "Epoch 332/350, Loss: 543.7639\n",
            "Epoch 333/350, Loss: 543.7626\n",
            "Epoch 334/350, Loss: 555.5584\n",
            "Epoch 335/350, Loss: 543.7839\n",
            "Epoch 336/350, Loss: 543.7541\n",
            "Epoch 337/350, Loss: 543.7653\n",
            "Epoch 338/350, Loss: 543.7479\n",
            "Epoch 339/350, Loss: 543.7498\n",
            "Epoch 340/350, Loss: 543.7613\n",
            "Epoch 341/350, Loss: 543.7820\n",
            "Epoch 342/350, Loss: 543.7787\n",
            "Epoch 343/350, Loss: 543.8072\n",
            "Epoch 344/350, Loss: 549.6387\n",
            "Epoch 345/350, Loss: 543.7746\n",
            "Epoch 346/350, Loss: 549.6442\n",
            "Epoch 347/350, Loss: 543.7834\n",
            "Epoch 348/350, Loss: 543.7356\n",
            "Epoch 349/350, Loss: 543.7652\n",
            "Epoch 350/350, Loss: 543.7747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment by Optimizer\n",
        "optimizer_results = []\n",
        "for opt_name, opt_fn in optimizers.items():\n",
        "    print(f\"\\nTraining with Optimizer: {opt_name}\")\n",
        "    model = RNNModel(input_size, 64, output_size).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = opt_fn(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_losses = train_model(model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "    predictions, actuals = evaluate_model(model, test_loader)\n",
        "\n",
        "    optimizer_results.append({\n",
        "        'optimizer': opt_name,\n",
        "        'train_losses': train_losses,\n",
        "        'predictions': predictions,\n",
        "        'actuals': actuals\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFVR6ei49FCU",
        "outputId": "db59b587-1658-408e-8ef5-1d502a2a4d49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Optimizer: SGD\n",
            "Epoch 1/50, Loss: 543.9439\n",
            "Epoch 2/50, Loss: 543.8221\n",
            "Epoch 3/50, Loss: 543.8209\n",
            "Epoch 4/50, Loss: 543.8326\n",
            "Epoch 5/50, Loss: 543.8026\n",
            "Epoch 6/50, Loss: 543.8191\n",
            "Epoch 7/50, Loss: 543.8366\n",
            "Epoch 8/50, Loss: 549.6849\n",
            "Epoch 9/50, Loss: 543.7990\n",
            "Epoch 10/50, Loss: 543.8082\n",
            "Epoch 11/50, Loss: 543.7829\n",
            "Epoch 12/50, Loss: 543.7906\n",
            "Epoch 13/50, Loss: 543.7772\n",
            "Epoch 14/50, Loss: 543.8159\n",
            "Epoch 15/50, Loss: 543.8043\n",
            "Epoch 16/50, Loss: 543.8044\n",
            "Epoch 17/50, Loss: 543.7818\n",
            "Epoch 18/50, Loss: 543.8019\n",
            "Epoch 19/50, Loss: 543.8155\n",
            "Epoch 20/50, Loss: 543.7926\n",
            "Epoch 21/50, Loss: 549.6802\n",
            "Epoch 22/50, Loss: 543.7952\n",
            "Epoch 23/50, Loss: 543.8071\n",
            "Epoch 24/50, Loss: 543.7895\n",
            "Epoch 25/50, Loss: 555.5855\n",
            "Epoch 26/50, Loss: 543.7961\n",
            "Epoch 27/50, Loss: 549.6793\n",
            "Epoch 28/50, Loss: 543.7903\n",
            "Epoch 29/50, Loss: 543.7933\n",
            "Epoch 30/50, Loss: 543.7774\n",
            "Epoch 31/50, Loss: 549.6931\n",
            "Epoch 32/50, Loss: 543.7963\n",
            "Epoch 33/50, Loss: 543.8018\n",
            "Epoch 34/50, Loss: 543.7962\n",
            "Epoch 35/50, Loss: 543.7861\n",
            "Epoch 36/50, Loss: 549.6936\n",
            "Epoch 37/50, Loss: 543.7868\n",
            "Epoch 38/50, Loss: 543.7734\n",
            "Epoch 39/50, Loss: 543.7795\n",
            "Epoch 40/50, Loss: 543.8036\n",
            "Epoch 41/50, Loss: 549.6655\n",
            "Epoch 42/50, Loss: 543.7745\n",
            "Epoch 43/50, Loss: 543.7926\n",
            "Epoch 44/50, Loss: 543.7800\n",
            "Epoch 45/50, Loss: 543.7761\n",
            "Epoch 46/50, Loss: 549.6675\n",
            "Epoch 47/50, Loss: 543.7671\n",
            "Epoch 48/50, Loss: 549.6605\n",
            "Epoch 49/50, Loss: 543.7700\n",
            "Epoch 50/50, Loss: 543.7776\n",
            "\n",
            "Training with Optimizer: RMSProp\n",
            "Epoch 1/50, Loss: 544.3940\n",
            "Epoch 2/50, Loss: 543.9660\n",
            "Epoch 3/50, Loss: 549.7986\n",
            "Epoch 4/50, Loss: 543.8585\n",
            "Epoch 5/50, Loss: 543.8872\n",
            "Epoch 6/50, Loss: 543.8542\n",
            "Epoch 7/50, Loss: 543.8848\n",
            "Epoch 8/50, Loss: 543.8587\n",
            "Epoch 9/50, Loss: 543.8481\n",
            "Epoch 10/50, Loss: 543.8660\n",
            "Epoch 11/50, Loss: 543.8396\n",
            "Epoch 12/50, Loss: 543.8355\n",
            "Epoch 13/50, Loss: 543.8414\n",
            "Epoch 14/50, Loss: 543.8388\n",
            "Epoch 15/50, Loss: 549.7025\n",
            "Epoch 16/50, Loss: 543.8475\n",
            "Epoch 17/50, Loss: 543.8029\n",
            "Epoch 18/50, Loss: 549.7089\n",
            "Epoch 19/50, Loss: 549.7488\n",
            "Epoch 20/50, Loss: 543.8520\n",
            "Epoch 21/50, Loss: 543.8230\n",
            "Epoch 22/50, Loss: 543.8231\n",
            "Epoch 23/50, Loss: 543.8193\n",
            "Epoch 24/50, Loss: 543.8196\n",
            "Epoch 25/50, Loss: 543.8732\n",
            "Epoch 26/50, Loss: 549.7238\n",
            "Epoch 27/50, Loss: 543.8309\n",
            "Epoch 28/50, Loss: 543.7933\n",
            "Epoch 29/50, Loss: 543.9369\n",
            "Epoch 30/50, Loss: 543.8243\n",
            "Epoch 31/50, Loss: 543.8007\n",
            "Epoch 32/50, Loss: 543.7986\n",
            "Epoch 33/50, Loss: 549.7013\n",
            "Epoch 34/50, Loss: 543.8432\n",
            "Epoch 35/50, Loss: 549.6838\n",
            "Epoch 36/50, Loss: 543.8321\n",
            "Epoch 37/50, Loss: 543.8067\n",
            "Epoch 38/50, Loss: 543.8127\n",
            "Epoch 39/50, Loss: 543.8331\n",
            "Epoch 40/50, Loss: 543.8207\n",
            "Epoch 41/50, Loss: 543.7991\n",
            "Epoch 42/50, Loss: 543.8174\n",
            "Epoch 43/50, Loss: 543.7993\n",
            "Epoch 44/50, Loss: 543.8007\n",
            "Epoch 45/50, Loss: 549.7379\n",
            "Epoch 46/50, Loss: 543.8210\n",
            "Epoch 47/50, Loss: 549.7346\n",
            "Epoch 48/50, Loss: 543.7845\n",
            "Epoch 49/50, Loss: 543.7911\n",
            "Epoch 50/50, Loss: 543.7973\n",
            "\n",
            "Training with Optimizer: Adam\n",
            "Epoch 1/50, Loss: 549.9812\n",
            "Epoch 2/50, Loss: 549.8313\n",
            "Epoch 3/50, Loss: 543.8404\n",
            "Epoch 4/50, Loss: 549.8047\n",
            "Epoch 5/50, Loss: 543.8374\n",
            "Epoch 6/50, Loss: 543.8102\n",
            "Epoch 7/50, Loss: 543.8221\n",
            "Epoch 8/50, Loss: 543.8415\n",
            "Epoch 9/50, Loss: 543.8105\n",
            "Epoch 10/50, Loss: 543.8250\n",
            "Epoch 11/50, Loss: 543.8197\n",
            "Epoch 12/50, Loss: 543.8032\n",
            "Epoch 13/50, Loss: 543.8130\n",
            "Epoch 14/50, Loss: 543.7965\n",
            "Epoch 15/50, Loss: 543.7971\n",
            "Epoch 16/50, Loss: 543.7952\n",
            "Epoch 17/50, Loss: 543.7961\n",
            "Epoch 18/50, Loss: 543.8305\n",
            "Epoch 19/50, Loss: 543.8271\n",
            "Epoch 20/50, Loss: 543.7933\n",
            "Epoch 21/50, Loss: 543.8100\n",
            "Epoch 22/50, Loss: 543.8126\n",
            "Epoch 23/50, Loss: 543.7954\n",
            "Epoch 24/50, Loss: 543.7912\n",
            "Epoch 25/50, Loss: 543.7977\n",
            "Epoch 26/50, Loss: 543.7883\n",
            "Epoch 27/50, Loss: 543.8046\n",
            "Epoch 28/50, Loss: 549.6870\n",
            "Epoch 29/50, Loss: 543.7883\n",
            "Epoch 30/50, Loss: 543.7937\n",
            "Epoch 31/50, Loss: 543.8217\n",
            "Epoch 32/50, Loss: 549.6512\n",
            "Epoch 33/50, Loss: 543.7978\n",
            "Epoch 34/50, Loss: 543.8133\n",
            "Epoch 35/50, Loss: 555.6083\n",
            "Epoch 36/50, Loss: 543.7602\n",
            "Epoch 37/50, Loss: 543.7840\n",
            "Epoch 38/50, Loss: 543.8186\n",
            "Epoch 39/50, Loss: 543.8392\n",
            "Epoch 40/50, Loss: 543.7991\n",
            "Epoch 41/50, Loss: 555.6196\n",
            "Epoch 42/50, Loss: 543.8128\n",
            "Epoch 43/50, Loss: 543.7909\n",
            "Epoch 44/50, Loss: 543.8064\n",
            "Epoch 45/50, Loss: 543.7608\n",
            "Epoch 46/50, Loss: 555.5445\n",
            "Epoch 47/50, Loss: 549.7063\n",
            "Epoch 48/50, Loss: 543.7756\n",
            "Epoch 49/50, Loss: 549.6777\n",
            "Epoch 50/50, Loss: 543.7713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "hidden_size_df = pd.DataFrame(hidden_size_results)\n",
        "hidden_size_df.to_csv(\"hidden_size_experiment.csv\", index=False)\n",
        "\n",
        "pooling_df = pd.DataFrame(pooling_results)\n",
        "pooling_df.to_csv(\"pooling_experiment.csv\", index=False)\n",
        "\n",
        "optimizer_df = pd.DataFrame(optimizer_results)\n",
        "optimizer_df.to_csv(\"optimizer_experiment.csv\", index=False)\n",
        "\n",
        "epoch_df = pd.DataFrame(epoch_results)\n",
        "epoch_df.to_csv(\"epoch_experiment.csv\", index=False)\n",
        "\n",
        "print(\"Separate experiments complete. Results saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEJ6SQBn9Fmy",
        "outputId": "4c4a13d8-9d0c-4bf7-ef0d-578b5d0addfc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Separate experiments complete. Results saved.\n"
          ]
        }
      ]
    }
  ]
}